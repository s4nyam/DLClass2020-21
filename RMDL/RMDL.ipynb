{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RMDL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5aJTtzXJmy0",
        "outputId": "c71a4d76-8256-4bb2-d7be-0cdd559ef379"
      },
      "source": [
        "# DOWNLOAD GLOVE\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import os, sys, tarfile\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "if sys.version_info >= (3, 0, 0):\n",
        "    import urllib.request as urllib  # ugly but works\n",
        "else:\n",
        "    import urllib\n",
        "\n",
        "print(sys.version_info)\n",
        "\n",
        "# image shape\n",
        "\n",
        "\n",
        "# path to the directory with the data\n",
        "DATA_DIR = '.\\Glove'\n",
        "\n",
        "# url of the binary data\n",
        "\n",
        "\n",
        "\n",
        "# path to the binary train file with image data\n",
        "\n",
        "\n",
        "def download_and_extract(data='Wikipedia'):\n",
        "    \"\"\"\n",
        "    Download and extract the GloVe\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    if data=='Wikipedia':\n",
        "        DATA_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "    elif data=='Common_Crawl_840B':\n",
        "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip'\n",
        "    elif data=='Common_Crawl_42B':\n",
        "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip'\n",
        "    elif data=='Twitter':\n",
        "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip'\n",
        "    else:\n",
        "        print(\"prameter should be Twitter, Common_Crawl_42B, Common_Crawl_840B, or Wikipedia\")\n",
        "        exit(0)\n",
        "\n",
        "\n",
        "    dest_directory = DATA_DIR\n",
        "    if not os.path.exists(dest_directory):\n",
        "        os.makedirs(dest_directory)\n",
        "    filename = DATA_URL.split('/')[-1]\n",
        "    filepath = os.path.join(dest_directory, filename)\n",
        "    print(filepath)\n",
        "\n",
        "    path = os.path.abspath(dest_directory)\n",
        "    if not os.path.exists(filepath):\n",
        "        def _progress(count, block_size, total_size):\n",
        "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
        "                                                          float(count * block_size) / float(total_size) * 100.0))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath)#, reporthook=_progress)\n",
        "\n",
        "\n",
        "        zip_ref = zipfile.ZipFile(filepath, 'r')\n",
        "        zip_ref.extractall(DATA_DIR)\n",
        "        zip_ref.close()\n",
        "    return path\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxdRuEg5M2CF",
        "outputId": "3e97e7b7-5af8-4143-90e3-4fad29f3ecb9"
      },
      "source": [
        "# DOWNLOAD WOS\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import os, sys, tarfile\n",
        "import numpy as np\n",
        "\n",
        "if sys.version_info >= (3, 0, 0):\n",
        "    import urllib.request as urllib  # ugly but works\n",
        "else:\n",
        "    import urllib\n",
        "\n",
        "print(sys.version_info)\n",
        "\n",
        "# image shape\n",
        "\n",
        "\n",
        "# path to the directory with the data\n",
        "DATA_DIR = '.\\data_WOS'\n",
        "\n",
        "# url of the binary data\n",
        "DATA_URL = 'http://kowsari.net/WebOfScience.tar.gz'\n",
        "\n",
        "\n",
        "# path to the binary train file with image data\n",
        "\n",
        "\n",
        "def download_and_extract():\n",
        "    \"\"\"\n",
        "    Download and extract the WOS datasets\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    dest_directory = DATA_DIR\n",
        "    if not os.path.exists(dest_directory):\n",
        "        os.makedirs(dest_directory)\n",
        "    filename = DATA_URL.split('/')[-1]\n",
        "    filepath = os.path.join(dest_directory, filename)\n",
        "\n",
        "\n",
        "    path = os.path.abspath(dest_directory)\n",
        "    if not os.path.exists(filepath):\n",
        "        def _progress(count, block_size, total_size):\n",
        "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
        "                                                          float(count * block_size) / float(total_size) * 100.0))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath, reporthook=_progress)\n",
        "\n",
        "        print('Downloaded', filename)\n",
        "\n",
        "        tarfile.open(filepath, 'r').extractall(dest_directory)\n",
        "    return path\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu_wC9mLM8J4"
      },
      "source": [
        "# MODEL\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "from keras.constraints import maxnorm\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.layers import Conv1D,MaxPooling2D, \\\n",
        "    MaxPooling1D, Embedding, Dropout,\\\n",
        "    GRU,TimeDistributed,Conv2D,\\\n",
        "    Activation,LSTM,Input\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Lambda\n",
        "from keras.layers.merge import Concatenate\n",
        "import tensorflow as tf\n",
        "from keras import optimizers\n",
        "import random\n",
        "\n",
        "def optimizors(random_optimizor):\n",
        "    if random_optimizor:\n",
        "        i = random.randint(1,3)\n",
        "        if i==0:\n",
        "            opt = optimizers.SGD()\n",
        "        elif i==1:\n",
        "            opt= optimizers.RMSprop()\n",
        "        elif i==2:\n",
        "            opt= optimizers.Adagrad()\n",
        "        elif i==3:\n",
        "            opt = optimizers.Adam()\n",
        "        elif i==4:\n",
        "            opt =optimizers.Nadam()\n",
        "        print(opt)\n",
        "    else:\n",
        "        opt= optimizers.Adam()\n",
        "    return opt\n",
        "\n",
        "\n",
        "\n",
        "def slice_batch(x, n_gpus, part):\n",
        "    \"\"\"\n",
        "    Divide the input batch into [n_gpus] slices, and obtain slice number [part].\n",
        "    i.e. if len(x)=10, then slice_batch(x, 2, 1) will return x[5:].\n",
        "    \"\"\"\n",
        "\n",
        "    sh = K.shape(x)\n",
        "    L = sh[0] // n_gpus\n",
        "    if part == n_gpus - 1:\n",
        "        return x[part*L:]\n",
        "    return x[part*L:(part+1)*L]\n",
        "\n",
        "def to_multi_gpu(model, n_gpus=2):\n",
        "    \"\"\"\n",
        "    Given a keras [model], return an equivalent model which parallelizes\n",
        "    the computation over [n_gpus] GPUs.\n",
        "\n",
        "    Each GPU gets a slice of the input batch, applies the model on that slice\n",
        "    and later the outputs of the models are concatenated to a single tensor,\n",
        "    hence the user sees a model that behaves the same as the original.\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "        x = Input(model.input_shape[1:], name=\"input1\")\n",
        "\n",
        "    towers = []\n",
        "    for g in range(n_gpus):\n",
        "        with tf.device('/gpu:' + str(g)):\n",
        "            slice_g = Lambda(slice_batch,\n",
        "                             lambda shape: shape,\n",
        "                             arguments={'n_gpus':n_gpus, 'part':g})(x)\n",
        "            towers.append(model(slice_g))\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "        merged = Concatenate(axis=0)(towers)\n",
        "\n",
        "    return Model(inputs=[x], outputs=[merged])\n",
        "\n",
        "\n",
        "def Build_Model_DNN_Image(shape, number_of_classes, sparse_categorical, min_hidden_layer_dnn,max_hidden_layer_dnn,\n",
        "                          min_nodes_dnn, max_nodes_dnn, random_optimizor, dropout):\n",
        "    '''\n",
        "    buildModel_DNN_image(shape, nClasses,sparse_categorical)\n",
        "    Build Deep neural networks Model for text classification\n",
        "    Shape is input feature space\n",
        "    nClasses is number of classes\n",
        "    '''\n",
        "\n",
        "    model = Sequential()\n",
        "    values = list(range(min_nodes_dnn,max_nodes_dnn))\n",
        "    Numberof_NOde = random.choice(values)\n",
        "    Lvalues = list(range(min_hidden_layer_dnn,max_hidden_layer_dnn))\n",
        "    nLayers =random.choice(Lvalues)\n",
        "    print(shape)\n",
        "    model.add(Flatten(input_shape=shape))\n",
        "    model.add(Dense(Numberof_NOde,activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    for i in range(0,nLayers-1):\n",
        "        Numberof_NOde = random.choice(values)\n",
        "        model.add(Dense(Numberof_NOde,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(number_of_classes, activation='softmax'))\n",
        "    model_tmp = model\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    return model,model_tmp\n",
        "\n",
        "\n",
        "def Build_Model_DNN_Text(shape, nClasses, sparse_categorical,\n",
        "                         min_hidden_layer_dnn, max_hidden_layer_dnn, min_nodes_dnn,\n",
        "                         max_nodes_dnn, random_optimizor, dropout):\n",
        "    \"\"\"\n",
        "    buildModel_DNN_Tex(shape, nClasses,sparse_categorical)\n",
        "    Build Deep neural networks Model for text classification\n",
        "    Shape is input feature space\n",
        "    nClasses is number of classes\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    layer = list(range(min_hidden_layer_dnn,max_hidden_layer_dnn))\n",
        "    node = list(range(min_nodes_dnn, max_nodes_dnn))\n",
        "\n",
        "\n",
        "    Numberof_NOde =  random.choice(node)\n",
        "    nLayers = random.choice(layer)\n",
        "\n",
        "    Numberof_NOde_old = Numberof_NOde\n",
        "    model.add(Dense(Numberof_NOde,input_dim=shape,activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    for i in range(0,nLayers):\n",
        "        Numberof_NOde = random.choice(node)\n",
        "        model.add(Dense(Numberof_NOde,input_dim=Numberof_NOde_old,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        Numberof_NOde_old = Numberof_NOde\n",
        "    model.add(Dense(nClasses, activation='softmax'))\n",
        "    model_tem = model\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    return model,model_tem\n",
        "\n",
        "\n",
        "def Build_Model_CNN_Image(shape, nclasses, sparse_categorical,\n",
        "                          min_hidden_layer_cnn, max_hidden_layer_cnn, min_nodes_cnn,\n",
        "                          max_nodes_cnn, random_optimizor, dropout):\n",
        "    \"\"\"\"\"\n",
        "    def Image_model_CNN(num_classes,shape):\n",
        "    num_classes is number of classes,\n",
        "    shape is (w,h,p)\n",
        "    \"\"\"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    values = list(range(min_nodes_cnn,max_nodes_cnn))\n",
        "    Layers = list(range(min_hidden_layer_cnn, max_hidden_layer_cnn))\n",
        "    Layer = random.choice(Layers)\n",
        "    Filter = random.choice(values)\n",
        "    model.add(Conv2D(Filter, (3, 3), padding='same', input_shape=shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(Filter, (3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    for i in range(0,Layer):\n",
        "        Filter = random.choice(values)\n",
        "        model.add(Conv2D(Filter, (3, 3),padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(nclasses,activation='softmax',kernel_constraint=maxnorm(3)))\n",
        "    model_tmp = model\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    return model,model_tmp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Build_Model_RNN_Image(shape,\n",
        "                          nclasses,\n",
        "                          sparse_categorical,\n",
        "                          min_nodes_rnn,\n",
        "                          max_nodes_rnn,\n",
        "                          random_optimizor,\n",
        "                          dropout):\n",
        "    \"\"\"\n",
        "        def Image_model_RNN(num_classes,shape):\n",
        "        num_classes is number of classes,\n",
        "        shape is (w,h,p)\n",
        "    \"\"\"\n",
        "    values = list(range(min_nodes_rnn,max_nodes_rnn))\n",
        "    node =  random.choice(values)\n",
        "\n",
        "    x = Input(shape=shape)\n",
        "\n",
        "    # Encodes a row of pixels using TimeDistributed Wrapper.\n",
        "    encoded_rows = TimeDistributed(LSTM(node,recurrent_dropout=dropout))(x)\n",
        "    node = random.choice(values)\n",
        "    # Encodes columns of encoded rows.\n",
        "    encoded_columns = LSTM(node,recurrent_dropout=dropout)(encoded_rows)\n",
        "\n",
        "    # Final predictions and model.\n",
        "    #prediction = Dense(256, activation='relu')(encoded_columns)\n",
        "    prediction = Dense(nclasses, activation='softmax')(encoded_columns)\n",
        "    model = Model(x, prediction)\n",
        "    model_tmp = model\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=optimizors(random_optimizor),\n",
        "                  metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizors(random_optimizor),\n",
        "                  metrics=['accuracy'])\n",
        "    return model,model_tmp\n",
        "\n",
        "\n",
        "def Build_Model_RNN_Text(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, sparse_categorical,\n",
        "                         min_hidden_layer_rnn, max_hidden_layer_rnn, min_nodes_rnn, max_nodes_rnn, random_optimizor, dropout):\n",
        "    \"\"\"\n",
        "    def buildModel_RNN(word_index, embeddings_index, nClasses, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, sparse_categorical):\n",
        "    word_index in word index ,\n",
        "    embeddings_index is embeddings index, look at data_helper.py\n",
        "    nClasses is number of classes,\n",
        "    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    values = list(range(min_nodes_rnn,max_nodes_rnn))\n",
        "    values_layer = list(range(min_hidden_layer_rnn,max_hidden_layer_rnn))\n",
        "\n",
        "    layer = random.choice(values_layer)\n",
        "    print(layer)\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) != len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n",
        "                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n",
        "                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True))\n",
        "\n",
        "    gru_node = random.choice(values)\n",
        "    print(gru_node)\n",
        "    for i in range(0,layer):\n",
        "        model.add(GRU(gru_node,return_sequences=True, recurrent_dropout=0.2))\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(GRU(gru_node, recurrent_dropout=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(nclasses, activation='softmax'))\n",
        "\n",
        "    model_tmp = model\n",
        "    #model = to_multi_gpu(model, 3)\n",
        "\n",
        "\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    return model,model_tmp\n",
        "\n",
        "\n",
        "def Build_Model_CNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, sparse_categorical,\n",
        "                       min_hidden_layer_cnn, max_hidden_layer_cnn, min_nodes_cnn, max_nodes_cnn, random_optimizor,\n",
        "                       dropout, simple_model=False):\n",
        "\n",
        "    \"\"\"\n",
        "        def buildModel_CNN(word_index,embeddings_index,nClasses,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,Complexity=0):\n",
        "        word_index in word index ,\n",
        "        embeddings_index is embeddings index, look at data_helper.py\n",
        "        nClasses is number of classes,\n",
        "        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n",
        "        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n",
        "        Complexity we have two different CNN model as follows\n",
        "        F=0 is simple CNN with [1 5] hidden layer\n",
        "        Complexity=2 is more complex model of CNN with filter_length of range [1 10]\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    if simple_model:\n",
        "        embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "        for word, i in word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                if len(embedding_matrix[i]) !=len(embedding_vector):\n",
        "                    print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
        "                                     \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
        "                                     \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                    exit(1)\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "        model.add(Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True))\n",
        "        values = list(range(min_nodes_cnn,max_nodes_cnn))\n",
        "        Layer = list(range(min_hidden_layer_cnn,max_hidden_layer_cnn))\n",
        "        Layer = random.choice(Layer)\n",
        "        for i in range(0,Layer):\n",
        "            Filter = random.choice(values)\n",
        "            model.add(Conv1D(Filter, 5, activation='relu'))\n",
        "            model.add(Dropout(dropout))\n",
        "            model.add(MaxPooling1D(5))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        Filter = random.choice(values)\n",
        "        model.add(Dense(Filter, activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        Filter = random.choice(values)\n",
        "        model.add(Dense(Filter, activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(nclasses, activation='softmax'))\n",
        "        model_tmp = model\n",
        "        #model = Model(sequence_input, preds)\n",
        "        if sparse_categorical:\n",
        "            model.compile(loss='sparse_categorical_crossentropy',\n",
        "                          optimizer=optimizors(random_optimizor),\n",
        "                          metrics=['accuracy'])\n",
        "        else:\n",
        "            model.compile(loss='categorical_crossentropy',\n",
        "                          optimizer=optimizors(random_optimizor),\n",
        "                          metrics=['accuracy'])\n",
        "    else:\n",
        "        embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "        for word, i in word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                if len(embedding_matrix[i]) !=len(embedding_vector):\n",
        "                    print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
        "                                     \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
        "                                     \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                    exit(1)\n",
        "\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        embedding_layer = Embedding(len(word_index) + 1,\n",
        "                                    EMBEDDING_DIM,\n",
        "                                    weights=[embedding_matrix],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=True)\n",
        "\n",
        "        # applying a more complex convolutional approach\n",
        "        convs = []\n",
        "        values_layer = list(range(min_hidden_layer_cnn,max_hidden_layer_cnn))\n",
        "        filter_sizes = []\n",
        "        layer = random.choice(values_layer)\n",
        "        print(\"Filter  \",layer)\n",
        "        for fl in range(0,layer):\n",
        "            filter_sizes.append((fl+2))\n",
        "\n",
        "        values_node = list(range(min_nodes_cnn,max_nodes_cnn))\n",
        "        node = random.choice(values_node)\n",
        "        print(\"Node  \", node)\n",
        "        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "        embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "        for fsz in filter_sizes:\n",
        "            l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
        "            l_pool = MaxPooling1D(5)(l_conv)\n",
        "            #l_pool = Dropout(0.25)(l_pool)\n",
        "            convs.append(l_pool)\n",
        "\n",
        "        l_merge = Concatenate(axis=1)(convs)\n",
        "        l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\n",
        "        l_cov1 = Dropout(dropout)(l_cov1)\n",
        "        l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "        l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\n",
        "        l_cov2 = Dropout(dropout)(l_cov2)\n",
        "        l_pool2 = MaxPooling1D(30)(l_cov2)\n",
        "        l_flat = Flatten()(l_pool2)\n",
        "        l_dense = Dense(1024, activation='relu')(l_flat)\n",
        "        l_dense = Dropout(dropout)(l_dense)\n",
        "        l_dense = Dense(512, activation='relu')(l_dense)\n",
        "        l_dense = Dropout(dropout)(l_dense)\n",
        "        preds = Dense(nclasses, activation='softmax')(l_dense)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model_tmp = model\n",
        "        if sparse_categorical:\n",
        "            model.compile(loss='sparse_categorical_crossentropy',\n",
        "                          optimizer=optimizors(random_optimizor),\n",
        "                          metrics=['accuracy'])\n",
        "        else:\n",
        "            model.compile(loss='categorical_crossentropy',\n",
        "                          optimizer=optimizors(random_optimizor),\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    return model,model_tmp\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dfDAkVCNGUV"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def setup():\n",
        "    np.set_printoptions(threshold=np.inf)\n",
        "    np.random.seed(7)\n",
        "    if not os.path.exists(\".\\weights\"):\n",
        "        os.makedirs(\".\\weights\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNL3Zt_1NIwM"
      },
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from pylab import *\n",
        "import itertools\n",
        "\n",
        "def RMDL_epoch(history_):\n",
        "    Number_of_models = len(history_)\n",
        "    caption=[]\n",
        "    for i in range(0,len(history_)):\n",
        "        caption.append('RDL '+str(i+1))\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    for i in range(0, Number_of_models):\n",
        "        plt.plot(history_[i].history['accuracy'])\n",
        "        plt.title('model train accuracy')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(0, Number_of_models):\n",
        "        plt.plot(history_[i].history['val_accuracy'])\n",
        "        plt.title('model test accuracy')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "\n",
        "    plt.show()\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    for i in range(0, Number_of_models):\n",
        "        # summarize history for loss\n",
        "        plt.plot(history_[i].history['loss'])\n",
        "\n",
        "        plt.title('model train loss ')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    plt.show()\n",
        "    plt.legend(\n",
        "        caption, loc='upper right')\n",
        "    for i in range(0, Number_of_models):\n",
        "        # summarize history for loss\n",
        "        plt.plot(history_[i].history['val_loss'])\n",
        "\n",
        "        plt.title('model loss test')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    #else:\n",
        "       # print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def accuracy(y_test,final_y):\n",
        "    np.set_printoptions(precision=2)\n",
        "    y_test_temp = np.argmax(y_test, axis=1)\n",
        "    F_score = accuracy_score(y_test_temp, final_y)\n",
        "    F1 = precision_recall_fscore_support(y_test_temp, final_y, average='micro')\n",
        "    F2 = precision_recall_fscore_support(y_test_temp, final_y, average='macro')\n",
        "    F3 = precision_recall_fscore_support(y_test_temp, final_y, average='weighted')\n",
        "    print(F_score)\n",
        "    print(F1)\n",
        "    print(F2)\n",
        "    print(F3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AdFP4SgNK4d",
        "outputId": "676efc6c-d80b-422e-95c8-b925f39ecd6f"
      },
      "source": [
        "!pip install RMDL\n",
        "# FOR IMAGE CLASSIFICATION\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from RMDL import Plot as Plot\n",
        "import gc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import collections\n",
        "from sklearn.metrics import f1_score\n",
        "from RMDL import BuildModel as BuildModel\n",
        "from RMDL import Global as G\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "np.random.seed(7)\n",
        "\n",
        "\n",
        "def Image_Classification(x_train, y_train, x_test, y_test, shape, batch_size=128,\n",
        "                         sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500], plot=False,\n",
        "                         min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n",
        "                         max_hidden_layer_rnn=5, min_nodes_rnn=32, max_nodes_rnn=128,\n",
        "                         min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n",
        "                         random_state=42, random_optimizor=True, dropout=0.05):\n",
        "    \"\"\"\n",
        "    def Image_Classification(x_train, y_train, x_test, y_test, shape, batch_size=128,\n",
        "                             sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500], plot=False,\n",
        "                             min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n",
        "                             min_hidden_layer_rnn=1, max_hidden_layer_rnn=5, min_nodes_rnn=32, max_nodes_rnn=128,\n",
        "                             min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n",
        "                             random_state=42, random_optimizor=True, dropout=0.05):\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "                x_train : string\n",
        "                    input X for training\n",
        "                y_train : int\n",
        "                    input Y for training\n",
        "                x_test : string\n",
        "                    input X for testing\n",
        "                x_test : int\n",
        "                    input Y for testing\n",
        "                shape : np.shape\n",
        "                    shape of image. The most common situation would be a 2D input with shape (batch_size, input_dim).\n",
        "                batch_size : Integer, , optional\n",
        "                    Number of samples per gradient update. If unspecified, it will default to 128\n",
        "                MAX_NB_WORDS: int, optional\n",
        "                    Maximum number of unique words in datasets, it will default to 75000.\n",
        "                GloVe_dir: String, optional\n",
        "                    Address of GloVe or any pre-trained directory, it will default to null which glove.6B.zip will be download.\n",
        "                GloVe_dir: String, optional\n",
        "                    Which version of GloVe or pre-trained word emending will be used, it will default to glove.6B.50d.txt.\n",
        "                    NOTE: if you use other version of GloVe EMBEDDING_DIM must be same dimensions.\n",
        "                sparse_categorical: bool.\n",
        "                    When target's dataset is (n,1) should be True, it will default to True.\n",
        "                random_deep: array of int [3], optional\n",
        "                    Number of ensembled model used in RMDL random_deep[0] is number of DNN, random_deep[1] is number of RNN, random_deep[0] is number of CNN, it will default to [3, 3, 3].\n",
        "                epochs: array of int [3], optional\n",
        "                    Number of epochs in each ensembled model used in RMDL epochs[0] is number of epochs used in DNN, epochs[1] is number of epochs used in RNN, epochs[0] is number of epochs used in CNN, it will default to [500, 500, 500].\n",
        "                plot: bool, optional\n",
        "                    True: shows confusion matrix and accuracy and loss\n",
        "                min_hidden_layer_dnn: Integer, optional\n",
        "                    Lower Bounds of hidden layers of DNN used in RMDL, it will default to 1.\n",
        "                max_hidden_layer_dnn: Integer, optional\n",
        "                    Upper bounds of hidden layers of DNN used in RMDL, it will default to 8.\n",
        "                min_nodes_dnn: Integer, optional\n",
        "                    Lower bounds of nodes in each layer of DNN used in RMDL, it will default to 128.\n",
        "                max_nodes_dnn: Integer, optional\n",
        "                    Upper bounds of nodes in each layer of DNN used in RMDL, it will default to 1024.\n",
        "                min_hidden_layer_rnn: Integer, optional\n",
        "                    Lower Bounds of hidden layers of RNN used in RMDL, it will default to 1.\n",
        "                min_hidden_layer_rnn: Integer, optional\n",
        "                    Upper Bounds of hidden layers of RNN used in RMDL, it will default to 5.\n",
        "                min_nodes_rnn: Integer, optional\n",
        "                    Lower bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 32.\n",
        "                max_nodes_rnn: Integer, optional\n",
        "                    Upper bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 128.\n",
        "                min_hidden_layer_cnn: Integer, optional\n",
        "                    Lower Bounds of hidden layers of CNN used in RMDL, it will default to 3.\n",
        "                max_hidden_layer_cnn: Integer, optional\n",
        "                    Upper Bounds of hidden layers of CNN used in RMDL, it will default to 10.\n",
        "                min_nodes_cnn: Integer, optional\n",
        "                    Lower bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 128.\n",
        "                min_nodes_cnn: Integer, optional\n",
        "                    Upper bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 512.\n",
        "                random_state : Integer, optional\n",
        "                    RandomState instance or None, optional (default=None)\n",
        "                    If Integer, random_state is the seed used by the random number generator;\n",
        "                random_optimizor : bool, optional\n",
        "                    If False, all models use adam optimizer. If True, all models use random optimizers. it will default to True\n",
        "                dropout: Float, optional\n",
        "                    between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "    if len(x_train) != len(y_train):\n",
        "        raise ValueError('shape of x_train and y_train must be equal'\n",
        "                         'The x_train has ' + str(len(x_train)) +\n",
        "                         'The x_train has' +\n",
        "                         str(len(y_train)))\n",
        "\n",
        "    if len(x_test) != len(y_test):\n",
        "        raise ValueError('shape of x_test and y_test must be equal '\n",
        "                         'The x_train has ' + str(len(x_test)) +\n",
        "                         'The y_test has ' +\n",
        "                         str(len(y_test)))\n",
        "\n",
        "    np.random.seed(random_state)\n",
        "    G.setup()\n",
        "    y_proba = []\n",
        "\n",
        "    score = []\n",
        "    history_ = []\n",
        "    if sparse_categorical:\n",
        "        number_of_classes = np.max(y_train)+1\n",
        "    else:\n",
        "        number_of_classes = np.shape(y_train)[0]\n",
        "\n",
        "    i =0\n",
        "    while i < random_deep[0]:\n",
        "        try:\n",
        "            print(\"DNN \", i, \"\\n\")\n",
        "            model_DNN, model_tmp = BuildModel.Build_Model_DNN_Image(shape,\n",
        "                                                                    number_of_classes,\n",
        "                                                                    sparse_categorical,\n",
        "                                                                    min_hidden_layer_dnn,\n",
        "                                                                    max_hidden_layer_dnn,\n",
        "                                                                    min_nodes_dnn,\n",
        "                                                                    max_nodes_dnn,\n",
        "                                                                    random_optimizor,\n",
        "                                                                    dropout)\n",
        "\n",
        "\n",
        "            filepath = \"weights\\weights_DNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath,\n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            history = model_DNN.fit(x_train, y_train,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    epochs=epochs[0],\n",
        "                                    batch_size=batch_size,\n",
        "                                    callbacks=callbacks_list,\n",
        "                                    verbose=2)\n",
        "            history_.append(history)\n",
        "            model_tmp.load_weights(filepath)\n",
        "\n",
        "            if sparse_categorical == 0:\n",
        "                model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                                  optimizer='adam',\n",
        "                                  metrics=['accuracy'])\n",
        "            else:\n",
        "                model_tmp.compile(loss='categorical_crossentropy',\n",
        "                                  optimizer='adam',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "            y_pr = model_tmp.predict_classes(x_test, batch_size=batch_size)\n",
        "            y_proba.append(np.array(y_pr))\n",
        "            score.append(accuracy_score(y_test, y_pr))\n",
        "            i = i + 1\n",
        "            del model_tmp\n",
        "            del model_DNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \"try to re-generate an other model\")\n",
        "            if max_hidden_layer_dnn > 3:\n",
        "                max_hidden_layer_dnn -= 1\n",
        "            if max_nodes_dnn > 256:\n",
        "                max_nodes_dnn -= 8\n",
        "\n",
        "\n",
        "    i =0\n",
        "    while i < random_deep[1]:\n",
        "        try:\n",
        "            print(\"RNN \", i, \"\\n\")\n",
        "            model_RNN, model_tmp = BuildModel.Build_Model_RNN_Image(shape,\n",
        "                                                                    number_of_classes,\n",
        "                                                                    sparse_categorical,\n",
        "                                                                    min_nodes_rnn,\n",
        "                                                                    max_nodes_rnn,\n",
        "                                                                    random_optimizor,\n",
        "                                                                    dropout)\n",
        "\n",
        "            filepath = \"weights\\weights_RNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath,\n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            history = model_RNN.fit(x_train, y_train,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    epochs=epochs[1],\n",
        "                                    batch_size=batch_size,\n",
        "                                    verbose=2,\n",
        "                                    callbacks=callbacks_list)\n",
        "\n",
        "            model_tmp.load_weights(filepath)\n",
        "            model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                              optimizer='rmsprop',\n",
        "                              metrics=['accuracy'])\n",
        "            history_.append(history)\n",
        "\n",
        "            y_pr = model_tmp.predict(x_test, batch_size=batch_size)\n",
        "            y_pr = np.argmax(y_pr, axis=1)\n",
        "            y_proba.append(np.array(y_pr))\n",
        "            score.append(accuracy_score(y_test, y_pr))\n",
        "            i = i+1\n",
        "            del model_tmp\n",
        "            del model_RNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \" try to re-generate another model\")\n",
        "            if max_hidden_layer_rnn > 3:\n",
        "                max_hidden_layer_rnn -= 1\n",
        "            if max_nodes_rnn > 64:\n",
        "                max_nodes_rnn -= 2\n",
        "\n",
        "    # reshape to be [samples][pixels][width][height]\n",
        "    i=0\n",
        "    while i < random_deep[2]:\n",
        "        try:\n",
        "            print(\"CNN \", i, \"\\n\")\n",
        "            model_CNN, model_tmp = BuildModel.Build_Model_CNN_Image(shape,\n",
        "                                                                    number_of_classes,\n",
        "                                                                    sparse_categorical,\n",
        "                                                                    min_hidden_layer_cnn,\n",
        "                                                                    max_hidden_layer_cnn,\n",
        "                                                                    min_nodes_cnn,\n",
        "                                                                    max_nodes_cnn,\n",
        "                                                                    random_optimizor,\n",
        "                                                                    dropout)\n",
        "\n",
        "            filepath = \"weights\\weights_CNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath, \n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1, \n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            history = model_CNN.fit(x_train, y_train,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    epochs=epochs[2],\n",
        "                                    batch_size=batch_size,\n",
        "                                    callbacks=callbacks_list,\n",
        "                                    verbose=2)\n",
        "            history_.append(history)\n",
        "            model_tmp.load_weights(filepath)\n",
        "            model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                              optimizer='adam',\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "            y_pr = model_tmp.predict_classes(x_test, batch_size=batch_size)\n",
        "            y_proba.append(np.array(y_pr))\n",
        "            score.append(accuracy_score(y_test, y_pr))\n",
        "            i = i+1\n",
        "            del model_tmp\n",
        "            del model_CNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \" try to re-generate another model\")\n",
        "            if max_hidden_layer_cnn > 5:\n",
        "                max_hidden_layer_cnn -= 1\n",
        "            if max_nodes_cnn > 128:\n",
        "                max_nodes_cnn -= 2\n",
        "                min_nodes_cnn -= 1\n",
        "\n",
        "\n",
        "\n",
        "    y_proba = np.array(y_proba).transpose()\n",
        "    print(y_proba.shape)\n",
        "    final_y = []\n",
        "    for i in range(0, y_proba.shape[0]):\n",
        "        a = np.array(y_proba[i, :])\n",
        "        a = collections.Counter(a).most_common()[0][0]\n",
        "        final_y.append(a)\n",
        "    F_score = accuracy_score(y_test, final_y)\n",
        "    F1 = f1_score(y_test, final_y, average='micro')\n",
        "    F2 = f1_score(y_test, final_y, average='macro')\n",
        "    F3 = f1_score(y_test, final_y, average='weighted')\n",
        "    cnf_matrix = confusion_matrix(y_test, final_y)\n",
        "    # Compute confusion matrix\n",
        "    np.set_printoptions(precision=2)\n",
        "    if plot:\n",
        "        # Plot non-normalized confusion matrix\n",
        "        classes = list(range(0,np.max(y_test)+1))\n",
        "        Plot.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                         title='Confusion matrix, without normalization')\n",
        "        Plot.plot_confusion_matrix(cnf_matrix, classes=classes,normalize=True,\n",
        "                              title='Confusion matrix, without normalization')\n",
        "\n",
        "    if plot:\n",
        "        Plot.RMDL_epoch(history_)\n",
        "\n",
        "    print(y_proba.shape)\n",
        "    print(\"Accuracy of\",len(score),\"models:\",score)\n",
        "    print(\"Accuracy:\",F_score)\n",
        "    print(\"F1_Micro:\",F1)\n",
        "    print(\"F1_Macro:\",F2)\n",
        "    print(\"F1_weighted:\",F3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting RMDL\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/1c/7911d9b8ea3a95983d19720560963b3b809af7308a46a111756606ed928f/RMDL-1.0.8-py2.py3-none-any.whl (44kB)\n",
            "\r\u001b[K     |                        | 10kB 19.3MB/s eta 0:00:01\r\u001b[K     |                 | 20kB 25.7MB/s eta 0:00:01\r\u001b[K     |          | 30kB 25.1MB/s eta 0:00:01\r\u001b[K     |  | 40kB 19.1MB/s eta 0:00:01\r\u001b[K     || 51kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from RMDL) (1.1.5)\n",
            "Requirement already satisfied: keras>=2.0.9 in /usr/local/lib/python3.6/dist-packages (from RMDL) (2.4.3)\n",
            "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from RMDL) (3.2.2)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.6/dist-packages (from RMDL) (3.2.5)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from RMDL) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from RMDL) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from RMDL) (2.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from RMDL) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->RMDL) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->RMDL) (2.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.9->RMDL) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.9->RMDL) (3.13)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->RMDL) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->RMDL) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->RMDL) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.4->RMDL) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.0->RMDL) (1.0.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (0.10.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (2.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (3.7.4.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (0.3.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.32.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (0.36.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (2.4.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow->RMDL) (53.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (3.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (1.25.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->RMDL) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->RMDL) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->RMDL) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->RMDL) (2.10)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow->RMDL) (3.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->RMDL) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->RMDL) (4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->RMDL) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->RMDL) (4.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow->RMDL) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->RMDL) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->RMDL) (0.4.8)\n",
            "Installing collected packages: RMDL\n",
            "Successfully installed RMDL-1.0.8\n",
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "377ni0W2NPjp"
      },
      "source": [
        "#TEXT CLASSIFICATION\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import collections\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from RMDL import BuildModel as BuildModel\n",
        "from RMDL.Download import Download_Glove as GloVe\n",
        "from RMDL import text_feature_extraction as txt\n",
        "from RMDL import Global as G\n",
        "from RMDL import Plot as Plot\n",
        "\n",
        "\n",
        "def Text_Classification(x_train, y_train, x_test,  y_test, batch_size=128,\n",
        "                        EMBEDDING_DIM=50,MAX_SEQUENCE_LENGTH = 500, MAX_NB_WORDS = 75000,\n",
        "                        GloVe_dir=\"\", GloVe_file = \"glove.6B.50d.txt\",\n",
        "                        sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500],  plot=False,\n",
        "                        min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n",
        "                        min_hidden_layer_rnn=1, max_hidden_layer_rnn=5, min_nodes_rnn=32,  max_nodes_rnn=128,\n",
        "                        min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n",
        "                        random_state=42, random_optimizor=True, dropout=0.5,no_of_classes=0):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Text_Classification(x_train, y_train, x_test,  y_test, batch_size=128,\n",
        "                        EMBEDDING_DIM=50,MAX_SEQUENCE_LENGTH = 500, MAX_NB_WORDS = 75000,\n",
        "                        GloVe_dir=\"\", GloVe_file = \"glove.6B.50d.txt\",\n",
        "                        sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500],  plot=False,\n",
        "                        min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n",
        "                        min_hidden_layer_rnn=1, max_hidden_layer_rnn=5, min_nodes_rnn=32,  max_nodes_rnn=128,\n",
        "                        min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n",
        "                        random_state=42, random_optimizor=True, dropout=0.5):\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            batch_size : Integer, , optional\n",
        "                Number of samples per gradient update. If unspecified, it will default to 128\n",
        "            MAX_NB_WORDS: int, optional\n",
        "                Maximum number of unique words in datasets, it will default to 75000.\n",
        "            GloVe_dir: String, optional\n",
        "                Address of GloVe or any pre-trained directory, it will default to null which glove.6B.zip will be download.\n",
        "            GloVe_dir: String, optional\n",
        "                Which version of GloVe or pre-trained word emending will be used, it will default to glove.6B.50d.txt.\n",
        "                NOTE: if you use other version of GloVe EMBEDDING_DIM must be same dimensions.\n",
        "            sparse_categorical: bool.\n",
        "                When target's dataset is (n,1) should be True, it will default to True.\n",
        "            random_deep: array of int [3], optional\n",
        "                Number of ensembled model used in RMDL random_deep[0] is number of DNN, random_deep[1] is number of RNN, random_deep[0] is number of CNN, it will default to [3, 3, 3].\n",
        "            epochs: array of int [3], optional\n",
        "                Number of epochs in each ensembled model used in RMDL epochs[0] is number of epochs used in DNN, epochs[1] is number of epochs used in RNN, epochs[0] is number of epochs used in CNN, it will default to [500, 500, 500].\n",
        "            plot: bool, optional\n",
        "                True: shows confusion matrix and accuracy and loss\n",
        "            min_hidden_layer_dnn: Integer, optional\n",
        "                Lower Bounds of hidden layers of DNN used in RMDL, it will default to 1.\n",
        "            max_hidden_layer_dnn: Integer, optional\n",
        "                Upper bounds of hidden layers of DNN used in RMDL, it will default to 8.\n",
        "            min_nodes_dnn: Integer, optional\n",
        "                Lower bounds of nodes in each layer of DNN used in RMDL, it will default to 128.\n",
        "            max_nodes_dnn: Integer, optional\n",
        "                Upper bounds of nodes in each layer of DNN used in RMDL, it will default to 1024.\n",
        "            min_hidden_layer_rnn: Integer, optional\n",
        "                Lower Bounds of hidden layers of RNN used in RMDL, it will default to 1.\n",
        "            min_hidden_layer_rnn: Integer, optional\n",
        "                Upper Bounds of hidden layers of RNN used in RMDL, it will default to 5.\n",
        "            min_nodes_rnn: Integer, optional\n",
        "                Lower bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 32.\n",
        "            max_nodes_rnn: Integer, optional\n",
        "                Upper bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 128.\n",
        "            min_hidden_layer_cnn: Integer, optional\n",
        "                Lower Bounds of hidden layers of CNN used in RMDL, it will default to 3.\n",
        "            max_hidden_layer_cnn: Integer, optional\n",
        "                Upper Bounds of hidden layers of CNN used in RMDL, it will default to 10.\n",
        "            min_nodes_cnn: Integer, optional\n",
        "                Lower bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 128.\n",
        "            min_nodes_cnn: Integer, optional\n",
        "                Upper bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 512.\n",
        "            random_state : Integer, optional\n",
        "                RandomState instance or None, optional (default=None)\n",
        "                If Integer, random_state is the seed used by the random number generator;\n",
        "            random_optimizor : bool, optional\n",
        "                If False, all models use adam optimizer. If True, all models use random optimizers. it will default to True\n",
        "            dropout: Float, optional\n",
        "                between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.\n",
        "\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "\n",
        "    glove_directory = GloVe_dir\n",
        "    GloVe_file = GloVe_file\n",
        "\n",
        "    print(\"Done1\")\n",
        "\n",
        "    GloVe_needed = random_deep[1] != 0 or random_deep[2] != 0\n",
        "    \n",
        "    # example_input  = [0,1,3]\n",
        "    # example_output :\n",
        "    # \n",
        "    # [[1 0 0 0]\n",
        "    #  [0 1 0 0]\n",
        "    #  [0 0 0 1]]\n",
        "    \n",
        "    def one_hot_encoder(value, label_data_):\n",
        "\n",
        "        label_data_[value] = 1\n",
        "\n",
        "        return label_data_\n",
        "\n",
        "    def _one_hot_values(labels_data):\n",
        "        encoded = [0] * len(labels_data)\n",
        "\n",
        "        for index_no, value in enumerate(labels_data):\n",
        "            max_value = [0] * (np.max(labels_data) + 1)\n",
        "\n",
        "            encoded[index_no] = one_hot_encoder(value, max_value)\n",
        "\n",
        "        return np.array(encoded)\n",
        "\n",
        "    if not isinstance(y_train[0], list) and not isinstance(y_train[0], np.ndarray) and not sparse_categorical:\n",
        "        #checking if labels are one hot or not otherwise dense_layer will give shape error \n",
        "        \n",
        "        print(\"converted_into_one_hot\")\n",
        "        y_train = _one_hot_values(y_train)\n",
        "        y_test = _one_hot_values(y_test)\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if GloVe_needed:\n",
        "        if glove_directory == \"\":\n",
        "            GloVe_directory = GloVe.download_and_extract()\n",
        "            GloVe_DIR = os.path.join(GloVe_directory, GloVe_file)\n",
        "        else:\n",
        "            GloVe_DIR = os.path.join(glove_directory, GloVe_file)\n",
        "\n",
        "        if not os.path.isfile(GloVe_DIR):\n",
        "            print(\"Could not find %s Set GloVe Directory in Global.py \", GloVe)\n",
        "            exit()\n",
        "\n",
        "    G.setup()\n",
        "    if random_deep[0] != 0:\n",
        "        x_train_tfidf, x_test_tfidf = txt.loadData(x_train, x_test,MAX_NB_WORDS=MAX_NB_WORDS)\n",
        "    if random_deep[1] != 0 or random_deep[2] != 0 :\n",
        "        print(GloVe_DIR)\n",
        "        x_train_embedded, x_test_embedded, word_index, embeddings_index = txt.loadData_Tokenizer(x_train, x_test,GloVe_DIR,MAX_NB_WORDS,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM)\n",
        "\n",
        "    del x_train\n",
        "    del x_test\n",
        "    gc.collect()\n",
        "\n",
        "    y_pr = []\n",
        "    History = []\n",
        "    score = []\n",
        "\n",
        "    if no_of_classes==0:\n",
        "        #checking no_of_classes\n",
        "        #np.max(data)+1 will not work for one_hot encoding labels\n",
        "        if sparse_categorical:\n",
        "            number_of_classes = np.max(y_train) + 1\n",
        "        else:\n",
        "            number_of_classes = len(y_train[0])\n",
        "    else:\n",
        "        number_of_classes = no_of_classes\n",
        "    print(number_of_classes)\n",
        "\n",
        "\n",
        "    i = 0\n",
        "    while i < random_deep[0]:\n",
        "        # model_DNN.append(Sequential())\n",
        "        try:\n",
        "            print(\"DNN \" + str(i))\n",
        "            filepath = \"weights\\weights_DNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath,\n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            model_DNN, model_tmp = BuildModel.Build_Model_DNN_Text(x_train_tfidf.shape[1],\n",
        "                                                                   number_of_classes,\n",
        "                                                                   sparse_categorical,\n",
        "                                                                   min_hidden_layer_dnn,\n",
        "                                                                   max_hidden_layer_dnn,\n",
        "                                                                   min_nodes_dnn,\n",
        "                                                                   max_nodes_dnn,\n",
        "                                                                   random_optimizor,\n",
        "                                                                   dropout)\n",
        "            model_history = model_DNN.fit(x_train_tfidf, y_train,\n",
        "                              validation_data=(x_test_tfidf, y_test),\n",
        "                              epochs=epochs[0],\n",
        "                              batch_size=batch_size,\n",
        "                              callbacks=callbacks_list,\n",
        "                              verbose=2)\n",
        "            History.append(model_history)\n",
        "\n",
        "            model_tmp.load_weights(filepath)\n",
        "            if sparse_categorical:\n",
        "                model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                                  optimizer='adam',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "                y_pr_ = model_tmp.predict_classes(x_test_tfidf,\n",
        "                                                  batch_size=batch_size)\n",
        "                y_pr.append(np.array(y_pr_))\n",
        "                score.append(accuracy_score(y_test, y_pr_))\n",
        "            else:\n",
        "                model_tmp.compile(loss='categorical_crossentropy',\n",
        "                                  optimizer='adam',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "                y_pr_ = model_tmp.predict(x_test_tfidf,\n",
        "                                          batch_size=batch_size)\n",
        "\n",
        "                y_pr_ = np.argmax(y_pr_, axis=1)\n",
        "                y_pr.append(np.array(y_pr_))\n",
        "                y_test_temp = np.argmax(y_test, axis=1)\n",
        "                score.append(accuracy_score(y_test_temp, y_pr_))\n",
        "            # print(y_proba)\n",
        "            i += 1\n",
        "            del model_tmp\n",
        "            del model_DNN\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            print(\"Check the Error \\n {} \".format(e))\n",
        "\n",
        "            print(\"Error in model\", i, \"try to re-generate another model\")\n",
        "            if max_hidden_layer_dnn > 3:\n",
        "                max_hidden_layer_dnn -= 1\n",
        "            if max_nodes_dnn > 256:\n",
        "                max_nodes_dnn -= 8\n",
        "\n",
        "    try:\n",
        "        del x_train_tfidf\n",
        "        del x_test_tfidf\n",
        "        gc.collect()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    i=0\n",
        "    while i < random_deep[1]:\n",
        "        try:\n",
        "            print(\"RNN \" + str(i))\n",
        "            filepath = \"weights\\weights_RNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath,\n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            model_RNN, model_tmp = BuildModel.Build_Model_RNN_Text(word_index,\n",
        "                                                                   embeddings_index,\n",
        "                                                                   number_of_classes,\n",
        "                                                                   MAX_SEQUENCE_LENGTH,\n",
        "                                                                   EMBEDDING_DIM,\n",
        "                                                                   sparse_categorical,\n",
        "                                                                   min_hidden_layer_rnn,\n",
        "                                                                   max_hidden_layer_rnn,\n",
        "                                                                   min_nodes_rnn,\n",
        "                                                                   max_nodes_rnn,\n",
        "                                                                   random_optimizor,\n",
        "                                                                   dropout)\n",
        "\n",
        "            model_history = model_RNN.fit(x_train_embedded, y_train,\n",
        "                              validation_data=(x_test_embedded, y_test),\n",
        "                              epochs=epochs[1],\n",
        "                              batch_size=batch_size,\n",
        "                              callbacks=callbacks_list,\n",
        "                              verbose=2)\n",
        "            History.append(model_history)\n",
        "\n",
        "            if sparse_categorical:\n",
        "                model_tmp.load_weights(filepath)\n",
        "                model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                                  optimizer='rmsprop',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "                y_pr_ = model_tmp.predict_classes(x_test_embedded, batch_size=batch_size)\n",
        "                y_pr.append(np.array(y_pr_))\n",
        "                score.append(accuracy_score(y_test, y_pr_))\n",
        "            else:\n",
        "                model_tmp.load_weights(filepath)\n",
        "                model_tmp.compile(loss='categorical_crossentropy',\n",
        "                                  optimizer='rmsprop',\n",
        "                                  metrics=['accuracy'])\n",
        "                y_pr_ = model_tmp.predict(x_test_embedded, batch_size=batch_size)\n",
        "                y_pr_ = np.argmax(y_pr_, axis=1)\n",
        "                y_pr.append(np.array(y_pr_))\n",
        "                y_test_temp = np.argmax(y_test, axis=1)\n",
        "                score.append(accuracy_score(y_test_temp, y_pr_))\n",
        "            i += 1\n",
        "            del model_tmp\n",
        "            del model_RNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \"try to re-generate another model\")\n",
        "            if max_hidden_layer_rnn > 3:\n",
        "                max_hidden_layer_rnn -= 1\n",
        "            if max_nodes_rnn > 64:\n",
        "                max_nodes_rnn -= 2\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    i = 0\n",
        "    while i < random_deep[2]:\n",
        "        try:\n",
        "            print(\"CNN \" + str(i))\n",
        "\n",
        "            model_CNN, model_tmp = BuildModel.Build_Model_CNN_Text(word_index,\n",
        "                                                                   embeddings_index,\n",
        "                                                                   number_of_classes,\n",
        "                                                                   MAX_SEQUENCE_LENGTH,\n",
        "                                                                   EMBEDDING_DIM,\n",
        "                                                                   sparse_categorical,\n",
        "                                                                   min_hidden_layer_cnn,\n",
        "                                                                   max_hidden_layer_cnn,\n",
        "                                                                   min_nodes_cnn,\n",
        "                                                                   max_nodes_cnn,\n",
        "                                                                   random_optimizor,\n",
        "                                                                   dropout)\n",
        "\n",
        "\n",
        "\n",
        "            filepath = \"weights\\weights_CNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            model_history = model_CNN.fit(x_train_embedded, y_train,\n",
        "                                          validation_data=(x_test_embedded, y_test),\n",
        "                                          epochs=epochs[2],\n",
        "                                          batch_size=batch_size,\n",
        "                                          callbacks=callbacks_list,\n",
        "                                          verbose=2)\n",
        "            History.append(model_history)\n",
        "\n",
        "            model_tmp.load_weights(filepath)\n",
        "            if sparse_categorical:\n",
        "                model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                                  optimizer='rmsprop',\n",
        "                                  metrics=['accuracy'])\n",
        "            else:\n",
        "                model_tmp.compile(loss='categorical_crossentropy',\n",
        "                                  optimizer='rmsprop',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "            y_pr_ = model_tmp.predict(x_test_embedded, batch_size=batch_size)\n",
        "            y_pr_ = np.argmax(y_pr_, axis=1)\n",
        "            y_pr.append(np.array(y_pr_))\n",
        "\n",
        "            if sparse_categorical:\n",
        "                score.append(accuracy_score(y_test, y_pr_))\n",
        "            else:\n",
        "                y_test_temp = np.argmax(y_test, axis=1)\n",
        "                score.append(accuracy_score(y_test_temp, y_pr_))\n",
        "            i += 1\n",
        "\n",
        "            del model_tmp\n",
        "            del model_CNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \"try to re-generate an other model\")\n",
        "            if max_hidden_layer_cnn > 5:\n",
        "                max_hidden_layer_cnn -= 1\n",
        "            if max_nodes_cnn > 128:\n",
        "                max_nodes_cnn -= 2\n",
        "                min_nodes_cnn -= 1\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    y_proba = np.array(y_pr).transpose()\n",
        "\n",
        "    final_y = []\n",
        "\n",
        "    for i in range(0, y_proba.shape[0]):\n",
        "        a = np.array(y_proba[i, :])\n",
        "        a = collections.Counter(a).most_common()[0][0]\n",
        "        final_y.append(a)\n",
        "    if sparse_categorical:\n",
        "        F_score = accuracy_score(y_test, final_y)\n",
        "        F1 = precision_recall_fscore_support(y_test, final_y, average='micro')\n",
        "        F2 = precision_recall_fscore_support(y_test, final_y, average='macro')\n",
        "        F3 = precision_recall_fscore_support(y_test, final_y, average='weighted')\n",
        "        cnf_matrix = confusion_matrix(y_test, final_y)\n",
        "        # Compute confusion matrix\n",
        "        # Plot non-normalized confusion matrix\n",
        "\n",
        "        if plot:\n",
        "            classes = list(range(0, np.max(y_test)+1))\n",
        "            Plot.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                                       title='Confusion matrix, without normalization')\n",
        "\n",
        "            # Plot normalized confusion matrix\n",
        "\n",
        "            Plot.plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True,\n",
        "                                       title='Normalized confusion matrix')\n",
        "    else:\n",
        "        y_test_temp = np.argmax(y_test, axis=1)\n",
        "        F_score = accuracy_score(y_test_temp, final_y)\n",
        "        F1 = precision_recall_fscore_support(y_test_temp, final_y, average='micro')\n",
        "        F2 = precision_recall_fscore_support(y_test_temp, final_y, average='macro')\n",
        "        F3 = precision_recall_fscore_support(y_test_temp, final_y, average='weighted')\n",
        "    if plot:\n",
        "        Plot.RMDL_epoch(History)\n",
        "    print(y_proba.shape)\n",
        "    print(\"Accuracy of\",len(score),\"models:\",score)\n",
        "    print(\"Accuracy:\",F_score)\n",
        "    print(\"F1_Micro:\",F1)\n",
        "    print(\"F1_Macro:\",F2)\n",
        "    print(\"F1_weighted:\",F3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t43THPpnNd-b",
        "outputId": "c68bce58-b99c-4af6-8219-add6a03da87c"
      },
      "source": [
        "# TEXT FEATURE EXTRACTION\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download(\"stopwords\")\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "\n",
        "def transliterate(line):\n",
        "    cedilla2latin = [[u'', u'A'], [u'', u'a'], [u'', u'C'], [u'', u'c'], [u'', u'S'], [u'', u's']]\n",
        "    tr = dict([(a[0], a[1]) for (a) in cedilla2latin])\n",
        "    new_line = \"\"\n",
        "    for letter in line:\n",
        "        if letter in tr:\n",
        "            new_line += tr[letter]\n",
        "        else:\n",
        "            new_line += letter\n",
        "    return new_line\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTnyErA5NiX2",
        "outputId": "86338ab4-dc3e-40d5-f253-63ee10805dc8"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train_D = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
        "X_test_D = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
        "X_train = X_train_D / 255.0\n",
        "X_test = X_test_D / 255.0\n",
        "number_of_classes = np.unique(y_train).shape[0]\n",
        "shape = (28, 28, 1)\n",
        "\n",
        "batch_size = 128\n",
        "sparse_categorical = 0\n",
        "n_epochs = [20, 20, 20]  ## DNN-RNN-CNN\n",
        "Random_Deep = [3, 3, 3]  ## DNN-RNN-CNN\n",
        "\n",
        "Image_Classification(X_train, y_train, X_test, y_test,shape,\n",
        "                     batch_size=batch_size,\n",
        "                     sparse_categorical=True,\n",
        "                     random_deep=Random_Deep,\n",
        "                     epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "DNN  0 \n",
            "\n",
            "(28, 28, 1)\n",
            "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7fa7b0034668>\n",
            "Epoch 1/20\n",
            "469/469 - 3s - loss: 2.2916 - accuracy: 0.1097 - val_loss: 2.2666 - val_accuracy: 0.1914\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.19140, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 2/20\n",
            "469/469 - 1s - loss: 2.2416 - accuracy: 0.2719 - val_loss: 2.1689 - val_accuracy: 0.5105\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.19140 to 0.51050, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 3/20\n",
            "469/469 - 1s - loss: 2.0265 - accuracy: 0.5060 - val_loss: 1.6444 - val_accuracy: 0.6171\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.51050 to 0.61710, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 4/20\n",
            "469/469 - 1s - loss: 1.3064 - accuracy: 0.6296 - val_loss: 0.8749 - val_accuracy: 0.7660\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.61710 to 0.76600, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 5/20\n",
            "469/469 - 1s - loss: 0.8150 - accuracy: 0.7484 - val_loss: 0.5816 - val_accuracy: 0.8257\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.76600 to 0.82570, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 6/20\n",
            "469/469 - 1s - loss: 0.6230 - accuracy: 0.8074 - val_loss: 0.4703 - val_accuracy: 0.8620\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.82570 to 0.86200, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 7/20\n",
            "469/469 - 1s - loss: 0.5314 - accuracy: 0.8382 - val_loss: 0.4092 - val_accuracy: 0.8801\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.86200 to 0.88010, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 8/20\n",
            "469/469 - 1s - loss: 0.4735 - accuracy: 0.8577 - val_loss: 0.3710 - val_accuracy: 0.8922\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.88010 to 0.89220, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 9/20\n",
            "469/469 - 1s - loss: 0.4320 - accuracy: 0.8724 - val_loss: 0.3424 - val_accuracy: 0.8997\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.89220 to 0.89970, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 10/20\n",
            "469/469 - 1s - loss: 0.4037 - accuracy: 0.8788 - val_loss: 0.3218 - val_accuracy: 0.9059\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.89970 to 0.90590, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 11/20\n",
            "469/469 - 1s - loss: 0.3779 - accuracy: 0.8895 - val_loss: 0.3029 - val_accuracy: 0.9125\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.90590 to 0.91250, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 12/20\n",
            "469/469 - 1s - loss: 0.3561 - accuracy: 0.8949 - val_loss: 0.2871 - val_accuracy: 0.9160\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.91250 to 0.91600, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 13/20\n",
            "469/469 - 1s - loss: 0.3388 - accuracy: 0.9002 - val_loss: 0.2758 - val_accuracy: 0.9202\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.91600 to 0.92020, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 14/20\n",
            "469/469 - 1s - loss: 0.3250 - accuracy: 0.9030 - val_loss: 0.2638 - val_accuracy: 0.9234\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.92020 to 0.92340, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 15/20\n",
            "469/469 - 1s - loss: 0.3112 - accuracy: 0.9090 - val_loss: 0.2553 - val_accuracy: 0.9258\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.92340 to 0.92580, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 16/20\n",
            "469/469 - 1s - loss: 0.3012 - accuracy: 0.9116 - val_loss: 0.2459 - val_accuracy: 0.9269\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.92580 to 0.92690, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 17/20\n",
            "469/469 - 1s - loss: 0.2854 - accuracy: 0.9156 - val_loss: 0.2366 - val_accuracy: 0.9315\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.92690 to 0.93150, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 18/20\n",
            "469/469 - 1s - loss: 0.2781 - accuracy: 0.9179 - val_loss: 0.2286 - val_accuracy: 0.9320\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.93150 to 0.93200, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 19/20\n",
            "469/469 - 1s - loss: 0.2675 - accuracy: 0.9217 - val_loss: 0.2219 - val_accuracy: 0.9351\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.93200 to 0.93510, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 20/20\n",
            "469/469 - 1s - loss: 0.2593 - accuracy: 0.9237 - val_loss: 0.2154 - val_accuracy: 0.9352\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.93510 to 0.93520, saving model to weights\\weights_DNN_0.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DNN  1 \n",
            "\n",
            "(28, 28, 1)\n",
            "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7fa744e06198>\n",
            "Epoch 1/20\n",
            "469/469 - 1s - loss: 2.1144 - accuracy: 0.4221 - val_loss: 1.8055 - val_accuracy: 0.6792\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.67920, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 2/20\n",
            "469/469 - 1s - loss: 1.4239 - accuracy: 0.7058 - val_loss: 0.9904 - val_accuracy: 0.7936\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.67920 to 0.79360, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 3/20\n",
            "469/469 - 1s - loss: 0.8483 - accuracy: 0.7958 - val_loss: 0.6399 - val_accuracy: 0.8498\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.79360 to 0.84980, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 4/20\n",
            "469/469 - 1s - loss: 0.6285 - accuracy: 0.8348 - val_loss: 0.5047 - val_accuracy: 0.8732\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.84980 to 0.87320, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 5/20\n",
            "469/469 - 1s - loss: 0.5303 - accuracy: 0.8546 - val_loss: 0.4371 - val_accuracy: 0.8858\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.87320 to 0.88580, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 6/20\n",
            "469/469 - 1s - loss: 0.4745 - accuracy: 0.8672 - val_loss: 0.3963 - val_accuracy: 0.8929\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.88580 to 0.89290, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 7/20\n",
            "469/469 - 1s - loss: 0.4370 - accuracy: 0.8750 - val_loss: 0.3683 - val_accuracy: 0.8982\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.89290 to 0.89820, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 8/20\n",
            "469/469 - 1s - loss: 0.4114 - accuracy: 0.8824 - val_loss: 0.3485 - val_accuracy: 0.9027\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.89820 to 0.90270, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 9/20\n",
            "469/469 - 1s - loss: 0.3925 - accuracy: 0.8868 - val_loss: 0.3339 - val_accuracy: 0.9058\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.90270 to 0.90580, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 10/20\n",
            "469/469 - 1s - loss: 0.3752 - accuracy: 0.8915 - val_loss: 0.3212 - val_accuracy: 0.9083\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.90580 to 0.90830, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 11/20\n",
            "469/469 - 1s - loss: 0.3649 - accuracy: 0.8935 - val_loss: 0.3104 - val_accuracy: 0.9116\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.90830 to 0.91160, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 12/20\n",
            "469/469 - 1s - loss: 0.3512 - accuracy: 0.8985 - val_loss: 0.3018 - val_accuracy: 0.9148\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.91160 to 0.91480, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 13/20\n",
            "469/469 - 1s - loss: 0.3418 - accuracy: 0.9012 - val_loss: 0.2939 - val_accuracy: 0.9173\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.91480 to 0.91730, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 14/20\n",
            "469/469 - 1s - loss: 0.3322 - accuracy: 0.9040 - val_loss: 0.2863 - val_accuracy: 0.9200\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.91730 to 0.92000, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 15/20\n",
            "469/469 - 1s - loss: 0.3241 - accuracy: 0.9055 - val_loss: 0.2799 - val_accuracy: 0.9212\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.92000 to 0.92120, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 16/20\n",
            "469/469 - 1s - loss: 0.3182 - accuracy: 0.9078 - val_loss: 0.2741 - val_accuracy: 0.9236\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.92120 to 0.92360, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 17/20\n",
            "469/469 - 1s - loss: 0.3092 - accuracy: 0.9102 - val_loss: 0.2683 - val_accuracy: 0.9241\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.92360 to 0.92410, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 18/20\n",
            "469/469 - 1s - loss: 0.3037 - accuracy: 0.9126 - val_loss: 0.2634 - val_accuracy: 0.9259\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.92410 to 0.92590, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 19/20\n",
            "469/469 - 1s - loss: 0.2977 - accuracy: 0.9135 - val_loss: 0.2589 - val_accuracy: 0.9262\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.92590 to 0.92620, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 20/20\n",
            "469/469 - 1s - loss: 0.2919 - accuracy: 0.9158 - val_loss: 0.2545 - val_accuracy: 0.9271\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.92620 to 0.92710, saving model to weights\\weights_DNN_1.hdf5\n",
            "DNN  2 \n",
            "\n",
            "(28, 28, 1)\n",
            "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7fa744334860>\n",
            "Epoch 1/20\n",
            "469/469 - 1s - loss: 1.8102 - accuracy: 0.5293 - val_loss: 1.3533 - val_accuracy: 0.7606\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.76060, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 2/20\n",
            "469/469 - 1s - loss: 1.1305 - accuracy: 0.7824 - val_loss: 0.9082 - val_accuracy: 0.8267\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.76060 to 0.82670, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 3/20\n",
            "469/469 - 1s - loss: 0.8362 - accuracy: 0.8235 - val_loss: 0.7140 - val_accuracy: 0.8544\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.82670 to 0.85440, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 4/20\n",
            "469/469 - 1s - loss: 0.6947 - accuracy: 0.8428 - val_loss: 0.6115 - val_accuracy: 0.8662\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.85440 to 0.86620, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 5/20\n",
            "469/469 - 1s - loss: 0.6129 - accuracy: 0.8550 - val_loss: 0.5484 - val_accuracy: 0.8755\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.86620 to 0.87550, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 6/20\n",
            "469/469 - 1s - loss: 0.5596 - accuracy: 0.8632 - val_loss: 0.5055 - val_accuracy: 0.8806\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.87550 to 0.88060, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 7/20\n",
            "469/469 - 1s - loss: 0.5226 - accuracy: 0.8693 - val_loss: 0.4739 - val_accuracy: 0.8849\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.88060 to 0.88490, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 8/20\n",
            "469/469 - 1s - loss: 0.4932 - accuracy: 0.8748 - val_loss: 0.4499 - val_accuracy: 0.8887\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.88490 to 0.88870, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 9/20\n",
            "469/469 - 1s - loss: 0.4710 - accuracy: 0.8786 - val_loss: 0.4306 - val_accuracy: 0.8919\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.88870 to 0.89190, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 10/20\n",
            "469/469 - 1s - loss: 0.4528 - accuracy: 0.8826 - val_loss: 0.4151 - val_accuracy: 0.8943\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.89190 to 0.89430, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 11/20\n",
            "469/469 - 1s - loss: 0.4384 - accuracy: 0.8849 - val_loss: 0.4017 - val_accuracy: 0.8975\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.89430 to 0.89750, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 12/20\n",
            "469/469 - 1s - loss: 0.4251 - accuracy: 0.8875 - val_loss: 0.3906 - val_accuracy: 0.8992\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.89750 to 0.89920, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 13/20\n",
            "469/469 - 1s - loss: 0.4143 - accuracy: 0.8903 - val_loss: 0.3807 - val_accuracy: 0.9009\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.89920 to 0.90090, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 14/20\n",
            "469/469 - 1s - loss: 0.4046 - accuracy: 0.8921 - val_loss: 0.3724 - val_accuracy: 0.9021\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.90090 to 0.90210, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 15/20\n",
            "469/469 - 1s - loss: 0.3960 - accuracy: 0.8941 - val_loss: 0.3648 - val_accuracy: 0.9039\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.90210 to 0.90390, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 16/20\n",
            "469/469 - 1s - loss: 0.3893 - accuracy: 0.8947 - val_loss: 0.3579 - val_accuracy: 0.9055\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.90390 to 0.90550, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 17/20\n",
            "469/469 - 1s - loss: 0.3820 - accuracy: 0.8969 - val_loss: 0.3518 - val_accuracy: 0.9064\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.90550 to 0.90640, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 18/20\n",
            "469/469 - 1s - loss: 0.3750 - accuracy: 0.8981 - val_loss: 0.3461 - val_accuracy: 0.9076\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.90640 to 0.90760, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 19/20\n",
            "469/469 - 1s - loss: 0.3693 - accuracy: 0.8999 - val_loss: 0.3409 - val_accuracy: 0.9094\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.90760 to 0.90940, saving model to weights\\weights_DNN_2.hdf5\n",
            "Epoch 20/20\n",
            "469/469 - 1s - loss: 0.3641 - accuracy: 0.8999 - val_loss: 0.3362 - val_accuracy: 0.9106\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.90940 to 0.91060, saving model to weights\\weights_DNN_2.hdf5\n",
            "RNN  0 \n",
            "\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7fa74415bb00>\n",
            "Epoch 1/20\n",
            "469/469 - 68s - loss: 0.7165 - accuracy: 0.7523 - val_loss: 0.2200 - val_accuracy: 0.9332\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.93320, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 2/20\n",
            "469/469 - 64s - loss: 0.1943 - accuracy: 0.9390 - val_loss: 0.1228 - val_accuracy: 0.9623\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.93320 to 0.96230, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 3/20\n",
            "469/469 - 63s - loss: 0.1171 - accuracy: 0.9643 - val_loss: 0.0867 - val_accuracy: 0.9733\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.96230 to 0.97330, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 4/20\n",
            "469/469 - 63s - loss: 0.0912 - accuracy: 0.9722 - val_loss: 0.0729 - val_accuracy: 0.9775\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.97330 to 0.97750, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 5/20\n",
            "469/469 - 64s - loss: 0.0746 - accuracy: 0.9771 - val_loss: 0.0705 - val_accuracy: 0.9780\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.97750 to 0.97800, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 6/20\n",
            "469/469 - 63s - loss: 0.0607 - accuracy: 0.9812 - val_loss: 0.0574 - val_accuracy: 0.9828\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.97800 to 0.98280, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 7/20\n",
            "469/469 - 64s - loss: 0.0551 - accuracy: 0.9829 - val_loss: 0.0589 - val_accuracy: 0.9788\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.98280\n",
            "Epoch 8/20\n",
            "469/469 - 63s - loss: 0.0487 - accuracy: 0.9853 - val_loss: 0.0422 - val_accuracy: 0.9859\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.98280 to 0.98590, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 9/20\n",
            "469/469 - 63s - loss: 0.0423 - accuracy: 0.9863 - val_loss: 0.0487 - val_accuracy: 0.9856\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.98590\n",
            "Epoch 10/20\n",
            "469/469 - 64s - loss: 0.0375 - accuracy: 0.9884 - val_loss: 0.0388 - val_accuracy: 0.9884\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.98590 to 0.98840, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 11/20\n",
            "469/469 - 63s - loss: 0.0397 - accuracy: 0.9876 - val_loss: 0.0405 - val_accuracy: 0.9869\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.98840\n",
            "Epoch 12/20\n",
            "469/469 - 63s - loss: 0.0352 - accuracy: 0.9891 - val_loss: 0.0448 - val_accuracy: 0.9859\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.98840\n",
            "Epoch 13/20\n",
            "469/469 - 62s - loss: 0.0313 - accuracy: 0.9898 - val_loss: 0.0406 - val_accuracy: 0.9891\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.98840 to 0.98910, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 14/20\n",
            "469/469 - 62s - loss: 0.0285 - accuracy: 0.9909 - val_loss: 0.0508 - val_accuracy: 0.9857\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.98910\n",
            "Epoch 15/20\n",
            "469/469 - 63s - loss: 0.0280 - accuracy: 0.9903 - val_loss: 0.0387 - val_accuracy: 0.9877\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.98910\n",
            "Epoch 16/20\n",
            "469/469 - 62s - loss: 0.0248 - accuracy: 0.9922 - val_loss: 0.0369 - val_accuracy: 0.9899\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.98910 to 0.98990, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 17/20\n",
            "469/469 - 62s - loss: 0.0251 - accuracy: 0.9920 - val_loss: 0.0410 - val_accuracy: 0.9887\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.98990\n",
            "Epoch 18/20\n",
            "469/469 - 62s - loss: 0.0200 - accuracy: 0.9932 - val_loss: 0.0431 - val_accuracy: 0.9869\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.98990\n",
            "Epoch 19/20\n",
            "469/469 - 62s - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.0382 - val_accuracy: 0.9899\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.98990\n",
            "Epoch 20/20\n",
            "469/469 - 62s - loss: 0.0215 - accuracy: 0.9930 - val_loss: 0.0370 - val_accuracy: 0.9892\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.98990\n",
            "RNN  1 \n",
            "\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7fa73866ccc0>\n",
            "Epoch 1/20\n",
            "469/469 - 66s - loss: 2.3019 - accuracy: 0.1159 - val_loss: 2.3015 - val_accuracy: 0.1147\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.11470, saving model to weights\\weights_RNN_1.hdf5\n",
            "Epoch 2/20\n",
            "469/469 - 62s - loss: 2.3014 - accuracy: 0.1035 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.11470\n",
            "Epoch 3/20\n",
            "469/469 - 62s - loss: 2.3010 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.11470\n",
            "Epoch 4/20\n",
            "469/469 - 63s - loss: 2.3008 - accuracy: 0.1124 - val_loss: 2.3006 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.11470\n",
            "Epoch 5/20\n",
            "469/469 - 63s - loss: 2.3006 - accuracy: 0.1124 - val_loss: 2.3005 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.11470\n",
            "Epoch 6/20\n",
            "469/469 - 62s - loss: 2.3004 - accuracy: 0.1124 - val_loss: 2.3003 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.11470\n",
            "Epoch 7/20\n",
            "469/469 - 62s - loss: 2.3003 - accuracy: 0.1124 - val_loss: 2.3001 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.11470\n",
            "Epoch 8/20\n",
            "469/469 - 62s - loss: 2.3001 - accuracy: 0.1124 - val_loss: 2.2999 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.11470\n",
            "Epoch 9/20\n",
            "469/469 - 63s - loss: 2.2999 - accuracy: 0.1124 - val_loss: 2.2997 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.11470\n",
            "Epoch 10/20\n",
            "469/469 - 63s - loss: 2.2996 - accuracy: 0.1124 - val_loss: 2.2994 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.11470\n",
            "Epoch 11/20\n",
            "469/469 - 63s - loss: 2.2994 - accuracy: 0.1124 - val_loss: 2.2992 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.11470\n",
            "Epoch 12/20\n",
            "469/469 - 62s - loss: 2.2991 - accuracy: 0.1124 - val_loss: 2.2989 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.11470\n",
            "Epoch 13/20\n",
            "469/469 - 62s - loss: 2.2987 - accuracy: 0.1124 - val_loss: 2.2985 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.11470\n",
            "Epoch 14/20\n",
            "469/469 - 63s - loss: 2.2983 - accuracy: 0.1124 - val_loss: 2.2980 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.11470\n",
            "Epoch 15/20\n",
            "469/469 - 63s - loss: 2.2978 - accuracy: 0.1125 - val_loss: 2.2974 - val_accuracy: 0.1136\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.11470\n",
            "Epoch 16/20\n",
            "469/469 - 62s - loss: 2.2972 - accuracy: 0.1126 - val_loss: 2.2967 - val_accuracy: 0.1144\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.11470\n",
            "Epoch 17/20\n",
            "469/469 - 62s - loss: 2.2963 - accuracy: 0.1137 - val_loss: 2.2957 - val_accuracy: 0.1154\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.11470 to 0.11540, saving model to weights\\weights_RNN_1.hdf5\n",
            "Epoch 18/20\n",
            "469/469 - 62s - loss: 2.2950 - accuracy: 0.1155 - val_loss: 2.2942 - val_accuracy: 0.1185\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.11540 to 0.11850, saving model to weights\\weights_RNN_1.hdf5\n",
            "Epoch 19/20\n",
            "469/469 - 62s - loss: 2.2932 - accuracy: 0.1218 - val_loss: 2.2919 - val_accuracy: 0.1309\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.11850 to 0.13090, saving model to weights\\weights_RNN_1.hdf5\n",
            "Epoch 20/20\n",
            "469/469 - 63s - loss: 2.2902 - accuracy: 0.1421 - val_loss: 2.2880 - val_accuracy: 0.1608\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.13090 to 0.16080, saving model to weights\\weights_RNN_1.hdf5\n",
            "RNN  2 \n",
            "\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7fa6f8393c18>\n",
            "Epoch 1/20\n",
            "469/469 - 66s - loss: 2.3021 - accuracy: 0.1248 - val_loss: 2.3018 - val_accuracy: 0.1141\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.11410, saving model to weights\\weights_RNN_2.hdf5\n",
            "Epoch 2/20\n",
            "469/469 - 63s - loss: 2.3016 - accuracy: 0.1125 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.11410\n",
            "Epoch 3/20\n",
            "469/469 - 62s - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.11410\n",
            "Epoch 4/20\n",
            "469/469 - 62s - loss: 2.3010 - accuracy: 0.1124 - val_loss: 2.3009 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.11410\n",
            "Epoch 5/20\n",
            "469/469 - 63s - loss: 2.3008 - accuracy: 0.1124 - val_loss: 2.3007 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.11410\n",
            "Epoch 6/20\n",
            "469/469 - 62s - loss: 2.3006 - accuracy: 0.1124 - val_loss: 2.3005 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.11410\n",
            "Epoch 7/20\n",
            "469/469 - 62s - loss: 2.3004 - accuracy: 0.1124 - val_loss: 2.3003 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.11410\n",
            "Epoch 8/20\n",
            "469/469 - 62s - loss: 2.3002 - accuracy: 0.1124 - val_loss: 2.3001 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.11410\n",
            "Epoch 9/20\n",
            "469/469 - 63s - loss: 2.3000 - accuracy: 0.1124 - val_loss: 2.2998 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.11410\n",
            "Epoch 10/20\n",
            "469/469 - 64s - loss: 2.2997 - accuracy: 0.1124 - val_loss: 2.2995 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.11410\n",
            "Epoch 11/20\n",
            "469/469 - 63s - loss: 2.2994 - accuracy: 0.1124 - val_loss: 2.2992 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.11410\n",
            "Epoch 12/20\n",
            "469/469 - 62s - loss: 2.2991 - accuracy: 0.1124 - val_loss: 2.2989 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.11410\n",
            "Epoch 13/20\n",
            "469/469 - 62s - loss: 2.2987 - accuracy: 0.1124 - val_loss: 2.2984 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.11410\n",
            "Epoch 14/20\n",
            "469/469 - 62s - loss: 2.2983 - accuracy: 0.1124 - val_loss: 2.2979 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.11410\n",
            "Epoch 15/20\n",
            "469/469 - 62s - loss: 2.2977 - accuracy: 0.1124 - val_loss: 2.2972 - val_accuracy: 0.1135\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.11410\n",
            "Epoch 16/20\n",
            "469/469 - 62s - loss: 2.2969 - accuracy: 0.1125 - val_loss: 2.2963 - val_accuracy: 0.1140\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.11410\n",
            "Epoch 17/20\n",
            "469/469 - 62s - loss: 2.2958 - accuracy: 0.1132 - val_loss: 2.2951 - val_accuracy: 0.1150\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.11410 to 0.11500, saving model to weights\\weights_RNN_2.hdf5\n",
            "Epoch 18/20\n",
            "469/469 - 62s - loss: 2.2944 - accuracy: 0.1153 - val_loss: 2.2933 - val_accuracy: 0.1184\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.11500 to 0.11840, saving model to weights\\weights_RNN_2.hdf5\n",
            "Epoch 19/20\n",
            "469/469 - 62s - loss: 2.2922 - accuracy: 0.1211 - val_loss: 2.2905 - val_accuracy: 0.1267\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.11840 to 0.12670, saving model to weights\\weights_RNN_2.hdf5\n",
            "Epoch 20/20\n",
            "469/469 - 62s - loss: 2.2885 - accuracy: 0.1325 - val_loss: 2.2856 - val_accuracy: 0.1435\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.12670 to 0.14350, saving model to weights\\weights_RNN_2.hdf5\n",
            "CNN  0 \n",
            "\n",
            "<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7fa6f7acc860>\n",
            "Epoch 1/20\n",
            "469/469 - 16s - loss: 0.2747 - accuracy: 0.9128 - val_loss: 0.0425 - val_accuracy: 0.9883\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.98830, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 2/20\n",
            "469/469 - 11s - loss: 0.0502 - accuracy: 0.9852 - val_loss: 0.0431 - val_accuracy: 0.9881\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.98830\n",
            "Epoch 3/20\n",
            "469/469 - 11s - loss: 0.0338 - accuracy: 0.9904 - val_loss: 0.0273 - val_accuracy: 0.9923\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.98830 to 0.99230, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 4/20\n",
            "469/469 - 11s - loss: 0.0282 - accuracy: 0.9921 - val_loss: 0.0374 - val_accuracy: 0.9895\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.99230\n",
            "Epoch 5/20\n",
            "469/469 - 11s - loss: 0.0224 - accuracy: 0.9934 - val_loss: 0.0341 - val_accuracy: 0.9927\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.99230 to 0.99270, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 6/20\n",
            "469/469 - 11s - loss: 0.0208 - accuracy: 0.9942 - val_loss: 0.0276 - val_accuracy: 0.9937\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.99270 to 0.99370, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 7/20\n",
            "469/469 - 11s - loss: 0.0180 - accuracy: 0.9950 - val_loss: 0.0323 - val_accuracy: 0.9927\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.99370\n",
            "Epoch 8/20\n",
            "469/469 - 11s - loss: 0.0164 - accuracy: 0.9957 - val_loss: 0.0267 - val_accuracy: 0.9938\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.99370 to 0.99380, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 9/20\n",
            "469/469 - 11s - loss: 0.0163 - accuracy: 0.9961 - val_loss: 0.0401 - val_accuracy: 0.9916\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.99380\n",
            "Epoch 10/20\n",
            "469/469 - 11s - loss: 0.0176 - accuracy: 0.9958 - val_loss: 0.0307 - val_accuracy: 0.9916\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.99380\n",
            "Epoch 11/20\n",
            "469/469 - 11s - loss: 0.0149 - accuracy: 0.9961 - val_loss: 0.0291 - val_accuracy: 0.9932\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.99380\n",
            "Epoch 12/20\n",
            "469/469 - 11s - loss: 0.0135 - accuracy: 0.9966 - val_loss: 0.0395 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.99380\n",
            "Epoch 13/20\n",
            "469/469 - 11s - loss: 0.0139 - accuracy: 0.9966 - val_loss: 0.0432 - val_accuracy: 0.9939\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.99380 to 0.99390, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 14/20\n",
            "469/469 - 11s - loss: 0.0148 - accuracy: 0.9966 - val_loss: 0.0574 - val_accuracy: 0.9930\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.99390\n",
            "Epoch 15/20\n",
            "469/469 - 11s - loss: 0.0150 - accuracy: 0.9967 - val_loss: 0.0368 - val_accuracy: 0.9928\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.99390\n",
            "Epoch 16/20\n",
            "469/469 - 11s - loss: 0.0137 - accuracy: 0.9972 - val_loss: 0.0478 - val_accuracy: 0.9938\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.99390\n",
            "Epoch 17/20\n",
            "469/469 - 11s - loss: 0.0121 - accuracy: 0.9970 - val_loss: 0.0509 - val_accuracy: 0.9941\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.99390 to 0.99410, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 18/20\n",
            "469/469 - 11s - loss: 0.0132 - accuracy: 0.9971 - val_loss: 0.0498 - val_accuracy: 0.9942\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.99410 to 0.99420, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 19/20\n",
            "469/469 - 11s - loss: 0.0137 - accuracy: 0.9973 - val_loss: 0.0324 - val_accuracy: 0.9909\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.99420\n",
            "Epoch 20/20\n",
            "469/469 - 11s - loss: 0.0127 - accuracy: 0.9972 - val_loss: 0.0546 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.99420\n",
            "CNN  1 \n",
            "\n",
            "<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7fa6f7739438>\n",
            "Epoch 1/20\n",
            "469/469 - 18s - loss: 0.3304 - accuracy: 0.8977 - val_loss: 0.0637 - val_accuracy: 0.9818\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.98180, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 2/20\n",
            "469/469 - 16s - loss: 0.0490 - accuracy: 0.9860 - val_loss: 0.0334 - val_accuracy: 0.9896\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.98180 to 0.98960, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 3/20\n",
            "469/469 - 16s - loss: 0.0365 - accuracy: 0.9903 - val_loss: 0.0367 - val_accuracy: 0.9893\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.98960\n",
            "Epoch 4/20\n",
            "469/469 - 16s - loss: 0.0293 - accuracy: 0.9927 - val_loss: 0.0235 - val_accuracy: 0.9925\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.98960 to 0.99250, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 5/20\n",
            "469/469 - 16s - loss: 0.0252 - accuracy: 0.9937 - val_loss: 0.0720 - val_accuracy: 0.9845\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.99250\n",
            "Epoch 6/20\n",
            "469/469 - 16s - loss: 0.0227 - accuracy: 0.9945 - val_loss: 0.0468 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.99250 to 0.99290, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 7/20\n",
            "469/469 - 16s - loss: 0.0232 - accuracy: 0.9947 - val_loss: 0.0345 - val_accuracy: 0.9920\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.99290\n",
            "Epoch 8/20\n",
            "469/469 - 16s - loss: 0.0231 - accuracy: 0.9946 - val_loss: 0.0328 - val_accuracy: 0.9921\n",
            "\n",
            "Epoch 00008: val_accuracy did not improve from 0.99290\n",
            "Epoch 9/20\n",
            "469/469 - 16s - loss: 0.0206 - accuracy: 0.9949 - val_loss: 0.0370 - val_accuracy: 0.9919\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.99290\n",
            "Epoch 10/20\n",
            "469/469 - 16s - loss: 0.0199 - accuracy: 0.9958 - val_loss: 0.0720 - val_accuracy: 0.9903\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.99290\n",
            "Epoch 11/20\n",
            "469/469 - 16s - loss: 0.0222 - accuracy: 0.9952 - val_loss: 0.0707 - val_accuracy: 0.9911\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.99290\n",
            "Epoch 12/20\n",
            "469/469 - 16s - loss: 0.0227 - accuracy: 0.9952 - val_loss: 0.0583 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.99290\n",
            "Epoch 13/20\n",
            "469/469 - 16s - loss: 0.0207 - accuracy: 0.9961 - val_loss: 0.0287 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.99290 to 0.99340, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 14/20\n",
            "469/469 - 16s - loss: 0.0225 - accuracy: 0.9957 - val_loss: 0.0444 - val_accuracy: 0.9921\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.99340\n",
            "Epoch 15/20\n",
            "469/469 - 16s - loss: 0.0205 - accuracy: 0.9960 - val_loss: 0.0286 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.99340\n",
            "Epoch 16/20\n",
            "469/469 - 16s - loss: 0.0240 - accuracy: 0.9958 - val_loss: 0.0518 - val_accuracy: 0.9925\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.99340\n",
            "Epoch 17/20\n",
            "469/469 - 16s - loss: 0.0200 - accuracy: 0.9963 - val_loss: 0.0984 - val_accuracy: 0.9918\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.99340\n",
            "Epoch 18/20\n",
            "469/469 - 16s - loss: 0.0222 - accuracy: 0.9961 - val_loss: 0.0321 - val_accuracy: 0.9928\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.99340\n",
            "Epoch 19/20\n",
            "469/469 - 16s - loss: 0.0221 - accuracy: 0.9967 - val_loss: 0.0444 - val_accuracy: 0.9929\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.99340\n",
            "Epoch 20/20\n",
            "469/469 - 16s - loss: 0.0208 - accuracy: 0.9962 - val_loss: 0.0379 - val_accuracy: 0.9930\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.99340\n",
            "CNN  2 \n",
            "\n",
            "<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7fa6f76eef60>\n",
            "Epoch 1/20\n",
            "469/469 - 19s - loss: 0.1700 - accuracy: 0.9456 - val_loss: 0.0438 - val_accuracy: 0.9863\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.98630, saving model to weights\\weights_CNN_2.hdf5\n",
            "Epoch 2/20\n",
            "469/469 - 17s - loss: 0.0442 - accuracy: 0.9865 - val_loss: 0.0488 - val_accuracy: 0.9839\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.98630\n",
            "Epoch 3/20\n",
            "469/469 - 17s - loss: 0.0311 - accuracy: 0.9910 - val_loss: 0.0321 - val_accuracy: 0.9902\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.98630 to 0.99020, saving model to weights\\weights_CNN_2.hdf5\n",
            "Epoch 4/20\n",
            "469/469 - 17s - loss: 0.0232 - accuracy: 0.9934 - val_loss: 0.0215 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.99020 to 0.99340, saving model to weights\\weights_CNN_2.hdf5\n",
            "Epoch 5/20\n",
            "469/469 - 17s - loss: 0.0198 - accuracy: 0.9942 - val_loss: 0.0267 - val_accuracy: 0.9920\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.99340\n",
            "Epoch 6/20\n",
            "469/469 - 17s - loss: 0.0181 - accuracy: 0.9950 - val_loss: 0.0323 - val_accuracy: 0.9908\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.99340\n",
            "Epoch 7/20\n",
            "469/469 - 17s - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.0366 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00007: val_accuracy did not improve from 0.99340\n",
            "Epoch 8/20\n",
            "469/469 - 17s - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.0207 - val_accuracy: 0.9946\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.99340 to 0.99460, saving model to weights\\weights_CNN_2.hdf5\n",
            "Epoch 9/20\n",
            "469/469 - 17s - loss: 0.0119 - accuracy: 0.9965 - val_loss: 0.0336 - val_accuracy: 0.9918\n",
            "\n",
            "Epoch 00009: val_accuracy did not improve from 0.99460\n",
            "Epoch 10/20\n",
            "469/469 - 17s - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.0325 - val_accuracy: 0.9913\n",
            "\n",
            "Epoch 00010: val_accuracy did not improve from 0.99460\n",
            "Epoch 11/20\n",
            "469/469 - 17s - loss: 0.0104 - accuracy: 0.9968 - val_loss: 0.0298 - val_accuracy: 0.9915\n",
            "\n",
            "Epoch 00011: val_accuracy did not improve from 0.99460\n",
            "Epoch 12/20\n",
            "469/469 - 17s - loss: 0.0076 - accuracy: 0.9976 - val_loss: 0.0256 - val_accuracy: 0.9942\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.99460\n",
            "Epoch 13/20\n",
            "469/469 - 17s - loss: 0.0080 - accuracy: 0.9977 - val_loss: 0.0285 - val_accuracy: 0.9937\n",
            "\n",
            "Epoch 00013: val_accuracy did not improve from 0.99460\n",
            "Epoch 14/20\n",
            "469/469 - 17s - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.0307 - val_accuracy: 0.9933\n",
            "\n",
            "Epoch 00014: val_accuracy did not improve from 0.99460\n",
            "Epoch 15/20\n",
            "469/469 - 17s - loss: 0.0095 - accuracy: 0.9972 - val_loss: 0.0430 - val_accuracy: 0.9916\n",
            "\n",
            "Epoch 00015: val_accuracy did not improve from 0.99460\n",
            "Epoch 16/20\n",
            "469/469 - 17s - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0358 - val_accuracy: 0.9934\n",
            "\n",
            "Epoch 00016: val_accuracy did not improve from 0.99460\n",
            "Epoch 17/20\n",
            "469/469 - 17s - loss: 0.0076 - accuracy: 0.9980 - val_loss: 0.0266 - val_accuracy: 0.9928\n",
            "\n",
            "Epoch 00017: val_accuracy did not improve from 0.99460\n",
            "Epoch 18/20\n",
            "469/469 - 17s - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.0386 - val_accuracy: 0.9913\n",
            "\n",
            "Epoch 00018: val_accuracy did not improve from 0.99460\n",
            "Epoch 19/20\n",
            "469/469 - 17s - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.0287 - val_accuracy: 0.9931\n",
            "\n",
            "Epoch 00019: val_accuracy did not improve from 0.99460\n",
            "Epoch 20/20\n",
            "469/469 - 17s - loss: 0.0078 - accuracy: 0.9976 - val_loss: 0.0310 - val_accuracy: 0.9935\n",
            "\n",
            "Epoch 00020: val_accuracy did not improve from 0.99460\n",
            "(10000, 9)\n",
            "(10000, 9)\n",
            "Accuracy of 9 models: [0.9352, 0.9271, 0.9106, 0.9899, 0.1608, 0.1435, 0.9942, 0.9934, 0.9946]\n",
            "Accuracy: 0.9874\n",
            "F1_Micro: 0.9874\n",
            "F1_Macro: 0.98746284969223\n",
            "F1_weighted: 0.9873994408273986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eWi9oneNxJj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RMDL IMDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5aJTtzXJmy0",
        "outputId": "5188ce9c-e2c6-4ee2-cb78-34592a8fa9e1"
      },
      "source": [
        "# DOWNLOAD GLOVE\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import os, sys, tarfile\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "if sys.version_info >= (3, 0, 0):\n",
        "    import urllib.request as urllib  # ugly but works\n",
        "else:\n",
        "    import urllib\n",
        "\n",
        "print(sys.version_info)\n",
        "\n",
        "# image shape\n",
        "\n",
        "\n",
        "# path to the directory with the data\n",
        "DATA_DIR = '.\\Glove'\n",
        "\n",
        "# url of the binary data\n",
        "\n",
        "\n",
        "\n",
        "# path to the binary train file with image data\n",
        "\n",
        "\n",
        "def download_and_extract(data='Wikipedia'):\n",
        "    \"\"\"\n",
        "    Download and extract the GloVe\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    if data=='Wikipedia':\n",
        "        DATA_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "    elif data=='Common_Crawl_840B':\n",
        "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip'\n",
        "    elif data=='Common_Crawl_42B':\n",
        "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip'\n",
        "    elif data=='Twitter':\n",
        "        DATA_URL = 'http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip'\n",
        "    else:\n",
        "        print(\"prameter should be Twitter, Common_Crawl_42B, Common_Crawl_840B, or Wikipedia\")\n",
        "        exit(0)\n",
        "\n",
        "\n",
        "    dest_directory = DATA_DIR\n",
        "    if not os.path.exists(dest_directory):\n",
        "        os.makedirs(dest_directory)\n",
        "    filename = DATA_URL.split('/')[-1]\n",
        "    filepath = os.path.join(dest_directory, filename)\n",
        "    print(filepath)\n",
        "\n",
        "    path = os.path.abspath(dest_directory)\n",
        "    if not os.path.exists(filepath):\n",
        "        def _progress(count, block_size, total_size):\n",
        "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
        "                                                          float(count * block_size) / float(total_size) * 100.0))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath)#, reporthook=_progress)\n",
        "\n",
        "\n",
        "        zip_ref = zipfile.ZipFile(filepath, 'r')\n",
        "        zip_ref.extractall(DATA_DIR)\n",
        "        zip_ref.close()\n",
        "    return path\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxdRuEg5M2CF",
        "outputId": "3b800b3c-9156-4d91-ac8c-1e6fc112dabf"
      },
      "source": [
        "# DOWNLOAD WOS\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import os, sys, tarfile\n",
        "import numpy as np\n",
        "\n",
        "if sys.version_info >= (3, 0, 0):\n",
        "    import urllib.request as urllib  # ugly but works\n",
        "else:\n",
        "    import urllib\n",
        "\n",
        "print(sys.version_info)\n",
        "\n",
        "# image shape\n",
        "\n",
        "\n",
        "# path to the directory with the data\n",
        "DATA_DIR = '.\\data_WOS'\n",
        "\n",
        "# url of the binary data\n",
        "DATA_URL = 'http://kowsari.net/WebOfScience.tar.gz'\n",
        "\n",
        "\n",
        "# path to the binary train file with image data\n",
        "\n",
        "\n",
        "def download_and_extract():\n",
        "    \"\"\"\n",
        "    Download and extract the WOS datasets\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    dest_directory = DATA_DIR\n",
        "    if not os.path.exists(dest_directory):\n",
        "        os.makedirs(dest_directory)\n",
        "    filename = DATA_URL.split('/')[-1]\n",
        "    filepath = os.path.join(dest_directory, filename)\n",
        "\n",
        "\n",
        "    path = os.path.abspath(dest_directory)\n",
        "    if not os.path.exists(filepath):\n",
        "        def _progress(count, block_size, total_size):\n",
        "            sys.stdout.write('\\rDownloading %s %.2f%%' % (filename,\n",
        "                                                          float(count * block_size) / float(total_size) * 100.0))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        filepath, _ = urllib.urlretrieve(DATA_URL, filepath, reporthook=_progress)\n",
        "\n",
        "        print('Downloaded', filename)\n",
        "\n",
        "        tarfile.open(filepath, 'r').extractall(dest_directory)\n",
        "    return path\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iu_wC9mLM8J4"
      },
      "source": [
        "# MODEL\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "from keras.constraints import maxnorm\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.layers import Conv1D,MaxPooling2D, \\\n",
        "    MaxPooling1D, Embedding, Dropout,\\\n",
        "    GRU,TimeDistributed,Conv2D,\\\n",
        "    Activation,LSTM,Input\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers.core import Lambda\n",
        "from keras.layers.merge import Concatenate\n",
        "import tensorflow as tf\n",
        "from keras import optimizers\n",
        "import random\n",
        "\n",
        "def optimizors(random_optimizor):\n",
        "    if random_optimizor:\n",
        "        i = random.randint(1,3)\n",
        "        if i==0:\n",
        "            opt = optimizers.SGD()\n",
        "        elif i==1:\n",
        "            opt= optimizers.RMSprop()\n",
        "        elif i==2:\n",
        "            opt= optimizers.Adagrad()\n",
        "        elif i==3:\n",
        "            opt = optimizers.Adam()\n",
        "        elif i==4:\n",
        "            opt =optimizers.Nadam()\n",
        "        print(opt)\n",
        "    else:\n",
        "        opt= optimizers.Adam()\n",
        "    return opt\n",
        "\n",
        "\n",
        "\n",
        "def slice_batch(x, n_gpus, part):\n",
        "    \"\"\"\n",
        "    Divide the input batch into [n_gpus] slices, and obtain slice number [part].\n",
        "    i.e. if len(x)=10, then slice_batch(x, 2, 1) will return x[5:].\n",
        "    \"\"\"\n",
        "\n",
        "    sh = K.shape(x)\n",
        "    L = sh[0] // n_gpus\n",
        "    if part == n_gpus - 1:\n",
        "        return x[part*L:]\n",
        "    return x[part*L:(part+1)*L]\n",
        "\n",
        "def to_multi_gpu(model, n_gpus=2):\n",
        "    \"\"\"\n",
        "    Given a keras [model], return an equivalent model which parallelizes\n",
        "    the computation over [n_gpus] GPUs.\n",
        "\n",
        "    Each GPU gets a slice of the input batch, applies the model on that slice\n",
        "    and later the outputs of the models are concatenated to a single tensor,\n",
        "    hence the user sees a model that behaves the same as the original.\n",
        "    \"\"\"\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "        x = Input(model.input_shape[1:], name=\"input1\")\n",
        "\n",
        "    towers = []\n",
        "    for g in range(n_gpus):\n",
        "        with tf.device('/gpu:' + str(g)):\n",
        "            slice_g = Lambda(slice_batch,\n",
        "                             lambda shape: shape,\n",
        "                             arguments={'n_gpus':n_gpus, 'part':g})(x)\n",
        "            towers.append(model(slice_g))\n",
        "\n",
        "    with tf.device('/cpu:0'):\n",
        "        merged = Concatenate(axis=0)(towers)\n",
        "\n",
        "    return Model(inputs=[x], outputs=[merged])\n",
        "\n",
        "\n",
        "def Build_Model_DNN_Image(shape, number_of_classes, sparse_categorical, min_hidden_layer_dnn,max_hidden_layer_dnn,\n",
        "                          min_nodes_dnn, max_nodes_dnn, random_optimizor, dropout):\n",
        "    '''\n",
        "    buildModel_DNN_image(shape, nClasses,sparse_categorical)\n",
        "    Build Deep neural networks Model for image classification\n",
        "    Shape is input feature space\n",
        "    nClasses is number of classes\n",
        "    '''\n",
        "\n",
        "    model = Sequential()\n",
        "    values = list(range(min_nodes_dnn,max_nodes_dnn))\n",
        "    Numberof_NOde = random.choice(values)\n",
        "    Lvalues = list(range(min_hidden_layer_dnn,max_hidden_layer_dnn))\n",
        "    nLayers =random.choice(Lvalues)\n",
        "    print(shape)\n",
        "    model.add(Flatten(input_shape=shape))\n",
        "    model.add(Dense(Numberof_NOde,activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    for i in range(0,nLayers-1):\n",
        "        Numberof_NOde = random.choice(values)\n",
        "        model.add(Dense(Numberof_NOde,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(number_of_classes, activation='softmax'))\n",
        "    model_tmp = model\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    return model,model_tmp\n",
        "\n",
        "\n",
        "def Build_Model_DNN_Text(shape, nClasses, sparse_categorical,\n",
        "                         min_hidden_layer_dnn, max_hidden_layer_dnn, min_nodes_dnn,\n",
        "                         max_nodes_dnn, random_optimizor, dropout):\n",
        "    \"\"\"\n",
        "    buildModel_DNN_Tex(shape, nClasses,sparse_categorical)\n",
        "    Build Deep neural networks Model for text classification\n",
        "    Shape is input feature space\n",
        "    nClasses is number of classes\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    layer = list(range(min_hidden_layer_dnn,max_hidden_layer_dnn))\n",
        "    node = list(range(min_nodes_dnn, max_nodes_dnn))\n",
        "\n",
        "\n",
        "    Numberof_NOde =  random.choice(node)\n",
        "    nLayers = random.choice(layer)\n",
        "\n",
        "    Numberof_NOde_old = Numberof_NOde\n",
        "    model.add(Dense(Numberof_NOde,input_dim=shape,activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    for i in range(0,nLayers):\n",
        "        Numberof_NOde = random.choice(node)\n",
        "        model.add(Dense(Numberof_NOde,input_dim=Numberof_NOde_old,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        Numberof_NOde_old = Numberof_NOde\n",
        "    model.add(Dense(nClasses, activation='softmax'))\n",
        "    model_tem = model\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    return model,model_tem\n",
        "\n",
        "\n",
        "def Build_Model_CNN_Image(shape, nclasses, sparse_categorical,\n",
        "                          min_hidden_layer_cnn, max_hidden_layer_cnn, min_nodes_cnn,\n",
        "                          max_nodes_cnn, random_optimizor, dropout):\n",
        "    \"\"\"\"\"\n",
        "    def Image_model_CNN(num_classes,shape):\n",
        "    num_classes is number of classes,\n",
        "    shape is (w,h,p)\n",
        "    \"\"\"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    values = list(range(min_nodes_cnn,max_nodes_cnn))\n",
        "    Layers = list(range(min_hidden_layer_cnn, max_hidden_layer_cnn))\n",
        "    Layer = random.choice(Layers)\n",
        "    Filter = random.choice(values)\n",
        "    model.add(Conv2D(Filter, (3, 3), padding='same', input_shape=shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(Filter, (3, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    for i in range(0,Layer):\n",
        "        Filter = random.choice(values)\n",
        "        model.add(Conv2D(Filter, (3, 3),padding='same'))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(nclasses,activation='softmax',kernel_constraint=maxnorm(3)))\n",
        "    model_tmp = model\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    return model,model_tmp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Build_Model_RNN_Image(shape,\n",
        "                          nclasses,\n",
        "                          sparse_categorical,\n",
        "                          min_nodes_rnn,\n",
        "                          max_nodes_rnn,\n",
        "                          random_optimizor,\n",
        "                          dropout):\n",
        "    \"\"\"\n",
        "        def Image_model_RNN(num_classes,shape):\n",
        "        num_classes is number of classes,\n",
        "        shape is (w,h,p)\n",
        "    \"\"\"\n",
        "    values = list(range(min_nodes_rnn,max_nodes_rnn))\n",
        "    node =  random.choice(values)\n",
        "\n",
        "    x = Input(shape=shape)\n",
        "\n",
        "    # Encodes a row of pixels using TimeDistributed Wrapper.\n",
        "    encoded_rows = TimeDistributed(LSTM(node,recurrent_dropout=dropout))(x)\n",
        "    node = random.choice(values)\n",
        "    # Encodes columns of encoded rows.\n",
        "    encoded_columns = LSTM(node,recurrent_dropout=dropout)(encoded_rows)\n",
        "\n",
        "    # Final predictions and model.\n",
        "    #prediction = Dense(256, activation='relu')(encoded_columns)\n",
        "    prediction = Dense(nclasses, activation='softmax')(encoded_columns)\n",
        "    model = Model(x, prediction)\n",
        "    model_tmp = model\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=optimizors(random_optimizor),\n",
        "                  metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizors(random_optimizor),\n",
        "                  metrics=['accuracy'])\n",
        "    return model,model_tmp\n",
        "\n",
        "\n",
        "def Build_Model_RNN_Text(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, sparse_categorical,\n",
        "                         min_hidden_layer_rnn, max_hidden_layer_rnn, min_nodes_rnn, max_nodes_rnn, random_optimizor, dropout):\n",
        "    \"\"\"\n",
        "    def buildModel_RNN(word_index, embeddings_index, nClasses, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, sparse_categorical):\n",
        "    word_index in word index ,\n",
        "    embeddings_index is embeddings index, look at data_helper.py\n",
        "    nClasses is number of classes,\n",
        "    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    values = list(range(min_nodes_rnn,max_nodes_rnn))\n",
        "    values_layer = list(range(min_hidden_layer_rnn,max_hidden_layer_rnn))\n",
        "\n",
        "    layer = random.choice(values_layer)\n",
        "    print(layer)\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) != len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n",
        "                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n",
        "                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True))\n",
        "\n",
        "    gru_node = random.choice(values)\n",
        "    print(gru_node)\n",
        "    for i in range(0,layer):\n",
        "        model.add(GRU(gru_node,return_sequences=True, recurrent_dropout=0.2))\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(GRU(gru_node, recurrent_dropout=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(nclasses, activation='softmax'))\n",
        "\n",
        "    model_tmp = model\n",
        "    #model = to_multi_gpu(model, 3)\n",
        "\n",
        "\n",
        "    if sparse_categorical:\n",
        "        model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizors(random_optimizor),\n",
        "                      metrics=['accuracy'])\n",
        "    return model,model_tmp\n",
        "\n",
        "\n",
        "def Build_Model_CNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, sparse_categorical,\n",
        "                       min_hidden_layer_cnn, max_hidden_layer_cnn, min_nodes_cnn, max_nodes_cnn, random_optimizor,\n",
        "                       dropout, simple_model=False):\n",
        "\n",
        "    \"\"\"\n",
        "        def buildModel_CNN(word_index,embeddings_index,nClasses,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM,Complexity=0):\n",
        "        word_index in word index ,\n",
        "        embeddings_index is embeddings index, look at data_helper.py\n",
        "        nClasses is number of classes,\n",
        "        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n",
        "        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n",
        "        Complexity we have two different CNN model as follows\n",
        "        F=0 is simple CNN with [1 5] hidden layer\n",
        "        Complexity=2 is more complex model of CNN with filter_length of range [1 10]\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    if simple_model:\n",
        "        embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "        for word, i in word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                if len(embedding_matrix[i]) !=len(embedding_vector):\n",
        "                    print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
        "                                     \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
        "                                     \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                    exit(1)\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "        model.add(Embedding(len(word_index) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True))\n",
        "        values = list(range(min_nodes_cnn,max_nodes_cnn))\n",
        "        Layer = list(range(min_hidden_layer_cnn,max_hidden_layer_cnn))\n",
        "        Layer = random.choice(Layer)\n",
        "        for i in range(0,Layer):\n",
        "            Filter = random.choice(values)\n",
        "            model.add(Conv1D(Filter, 5, activation='relu'))\n",
        "            model.add(Dropout(dropout))\n",
        "            model.add(MaxPooling1D(5))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        Filter = random.choice(values)\n",
        "        model.add(Dense(Filter, activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "        Filter = random.choice(values)\n",
        "        model.add(Dense(Filter, activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        model.add(Dense(nclasses, activation='softmax'))\n",
        "        model_tmp = model\n",
        "        #model = Model(sequence_input, preds)\n",
        "        if sparse_categorical:\n",
        "            model.compile(loss='sparse_categorical_crossentropy',\n",
        "                          optimizer=optimizors(random_optimizor),\n",
        "                          metrics=['accuracy'])\n",
        "        else:\n",
        "            model.compile(loss='categorical_crossentropy',\n",
        "                          optimizer=optimizors(random_optimizor),\n",
        "                          metrics=['accuracy'])\n",
        "    else:\n",
        "        embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "        for word, i in word_index.items():\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                if len(embedding_matrix[i]) !=len(embedding_vector):\n",
        "                    print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
        "                                     \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
        "                                     \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                    exit(1)\n",
        "\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        embedding_layer = Embedding(len(word_index) + 1,\n",
        "                                    EMBEDDING_DIM,\n",
        "                                    weights=[embedding_matrix],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=True)\n",
        "\n",
        "        # applying a more complex convolutional approach\n",
        "        convs = []\n",
        "        values_layer = list(range(min_hidden_layer_cnn,max_hidden_layer_cnn))\n",
        "        filter_sizes = []\n",
        "        layer = random.choice(values_layer)\n",
        "        print(\"Filter  \",layer)\n",
        "        for fl in range(0,layer):\n",
        "            filter_sizes.append((fl+2))\n",
        "\n",
        "        values_node = list(range(min_nodes_cnn,max_nodes_cnn))\n",
        "        node = random.choice(values_node)\n",
        "        print(\"Node  \", node)\n",
        "        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "        embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "        for fsz in filter_sizes:\n",
        "            l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
        "            l_pool = MaxPooling1D(5)(l_conv)\n",
        "            #l_pool = Dropout(0.25)(l_pool)\n",
        "            convs.append(l_pool)\n",
        "\n",
        "        l_merge = Concatenate(axis=1)(convs)\n",
        "        l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\n",
        "        l_cov1 = Dropout(dropout)(l_cov1)\n",
        "        l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "        l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\n",
        "        l_cov2 = Dropout(dropout)(l_cov2)\n",
        "        l_pool2 = MaxPooling1D(30)(l_cov2)\n",
        "        l_flat = Flatten()(l_pool2)\n",
        "        l_dense = Dense(1024, activation='relu')(l_flat)\n",
        "        l_dense = Dropout(dropout)(l_dense)\n",
        "        l_dense = Dense(512, activation='relu')(l_dense)\n",
        "        l_dense = Dropout(dropout)(l_dense)\n",
        "        preds = Dense(nclasses, activation='softmax')(l_dense)\n",
        "        model = Model(sequence_input, preds)\n",
        "        model_tmp = model\n",
        "        if sparse_categorical:\n",
        "            model.compile(loss='sparse_categorical_crossentropy',\n",
        "                          optimizer=optimizors(random_optimizor),\n",
        "                          metrics=['accuracy'])\n",
        "        else:\n",
        "            model.compile(loss='categorical_crossentropy',\n",
        "                          optimizer=optimizors(random_optimizor),\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    return model,model_tmp\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dfDAkVCNGUV"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def setup():\n",
        "    np.set_printoptions(threshold=np.inf)\n",
        "    np.random.seed(7)\n",
        "    if not os.path.exists(\".\\weights\"):\n",
        "        os.makedirs(\".\\weights\")\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNL3Zt_1NIwM"
      },
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from pylab import *\n",
        "import itertools\n",
        "\n",
        "def RMDL_epoch(history_):\n",
        "    Number_of_models = len(history_)\n",
        "    caption=[]\n",
        "    for i in range(0,len(history_)):\n",
        "        caption.append('RDL '+str(i+1))\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    for i in range(0, Number_of_models):\n",
        "        plt.plot(history_[i].history['accuracy'])\n",
        "        plt.title('model train accuracy')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(0, Number_of_models):\n",
        "        plt.plot(history_[i].history['val_accuracy'])\n",
        "        plt.title('model test accuracy')\n",
        "        plt.ylabel('accuracy')\n",
        "        plt.xlabel('epoch')\n",
        "\n",
        "    plt.show()\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    for i in range(0, Number_of_models):\n",
        "        # summarize history for loss\n",
        "        plt.plot(history_[i].history['loss'])\n",
        "\n",
        "        plt.title('model train loss ')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    plt.show()\n",
        "    plt.legend(\n",
        "        caption, loc='upper right')\n",
        "    for i in range(0, Number_of_models):\n",
        "        # summarize history for loss\n",
        "        plt.plot(history_[i].history['val_loss'])\n",
        "\n",
        "        plt.title('model loss test')\n",
        "        plt.ylabel('loss')\n",
        "        plt.xlabel('epoch')\n",
        "    plt.legend(caption, loc='upper right')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        #print(\"Normalized confusion matrix\")\n",
        "    #else:\n",
        "       # print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def accuracy(y_test,final_y):\n",
        "    np.set_printoptions(precision=2)\n",
        "    y_test_temp = np.argmax(y_test, axis=1)\n",
        "    F_score = accuracy_score(y_test_temp, final_y)\n",
        "    F1 = precision_recall_fscore_support(y_test_temp, final_y, average='micro')\n",
        "    F2 = precision_recall_fscore_support(y_test_temp, final_y, average='macro')\n",
        "    F3 = precision_recall_fscore_support(y_test_temp, final_y, average='weighted')\n",
        "    print(F_score)\n",
        "    print(F1)\n",
        "    print(F2)\n",
        "    print(F3)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AdFP4SgNK4d",
        "outputId": "a3ac9653-f9b9-476c-824d-b7949abe5371"
      },
      "source": [
        "!pip install RMDL\n",
        "# FOR IMAGE CLASSIFICATION\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from RMDL import Plot as Plot\n",
        "import gc\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import collections\n",
        "from sklearn.metrics import f1_score\n",
        "from RMDL import BuildModel as BuildModel\n",
        "from RMDL import Global as G\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "np.random.seed(7)\n",
        "\n",
        "\n",
        "def Image_Classification(x_train, y_train, x_test, y_test, shape, batch_size=128,\n",
        "                         sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500], plot=False,\n",
        "                         min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n",
        "                         max_hidden_layer_rnn=5, min_nodes_rnn=32, max_nodes_rnn=128,\n",
        "                         min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n",
        "                         random_state=42, random_optimizor=True, dropout=0.05):\n",
        "    \"\"\"\n",
        "    def Image_Classification(x_train, y_train, x_test, y_test, shape, batch_size=128,\n",
        "                             sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500], plot=False,\n",
        "                             min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n",
        "                             min_hidden_layer_rnn=1, max_hidden_layer_rnn=5, min_nodes_rnn=32, max_nodes_rnn=128,\n",
        "                             min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n",
        "                             random_state=42, random_optimizor=True, dropout=0.05):\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "                x_train : string\n",
        "                    input X for training\n",
        "                y_train : int\n",
        "                    input Y for training\n",
        "                x_test : string\n",
        "                    input X for testing\n",
        "                x_test : int\n",
        "                    input Y for testing\n",
        "                shape : np.shape\n",
        "                    shape of image. The most common situation would be a 2D input with shape (batch_size, input_dim).\n",
        "                batch_size : Integer, , optional\n",
        "                    Number of samples per gradient update. If unspecified, it will default to 128\n",
        "                MAX_NB_WORDS: int, optional\n",
        "                    Maximum number of unique words in datasets, it will default to 75000.\n",
        "                GloVe_dir: String, optional\n",
        "                    Address of GloVe or any pre-trained directory, it will default to null which glove.6B.zip will be download.\n",
        "                GloVe_dir: String, optional\n",
        "                    Which version of GloVe or pre-trained word emending will be used, it will default to glove.6B.50d.txt.\n",
        "                    NOTE: if you use other version of GloVe EMBEDDING_DIM must be same dimensions.\n",
        "                sparse_categorical: bool.\n",
        "                    When target's dataset is (n,1) should be True, it will default to True.\n",
        "                random_deep: array of int [3], optional\n",
        "                    Number of ensembled model used in RMDL random_deep[0] is number of DNN, random_deep[1] is number of RNN, random_deep[0] is number of CNN, it will default to [3, 3, 3].\n",
        "                epochs: array of int [3], optional\n",
        "                    Number of epochs in each ensembled model used in RMDL epochs[0] is number of epochs used in DNN, epochs[1] is number of epochs used in RNN, epochs[0] is number of epochs used in CNN, it will default to [500, 500, 500].\n",
        "                plot: bool, optional\n",
        "                    True: shows confusion matrix and accuracy and loss\n",
        "                min_hidden_layer_dnn: Integer, optional\n",
        "                    Lower Bounds of hidden layers of DNN used in RMDL, it will default to 1.\n",
        "                max_hidden_layer_dnn: Integer, optional\n",
        "                    Upper bounds of hidden layers of DNN used in RMDL, it will default to 8.\n",
        "                min_nodes_dnn: Integer, optional\n",
        "                    Lower bounds of nodes in each layer of DNN used in RMDL, it will default to 128.\n",
        "                max_nodes_dnn: Integer, optional\n",
        "                    Upper bounds of nodes in each layer of DNN used in RMDL, it will default to 1024.\n",
        "                min_hidden_layer_rnn: Integer, optional\n",
        "                    Lower Bounds of hidden layers of RNN used in RMDL, it will default to 1.\n",
        "                min_hidden_layer_rnn: Integer, optional\n",
        "                    Upper Bounds of hidden layers of RNN used in RMDL, it will default to 5.\n",
        "                min_nodes_rnn: Integer, optional\n",
        "                    Lower bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 32.\n",
        "                max_nodes_rnn: Integer, optional\n",
        "                    Upper bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 128.\n",
        "                min_hidden_layer_cnn: Integer, optional\n",
        "                    Lower Bounds of hidden layers of CNN used in RMDL, it will default to 3.\n",
        "                max_hidden_layer_cnn: Integer, optional\n",
        "                    Upper Bounds of hidden layers of CNN used in RMDL, it will default to 10.\n",
        "                min_nodes_cnn: Integer, optional\n",
        "                    Lower bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 128.\n",
        "                min_nodes_cnn: Integer, optional\n",
        "                    Upper bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 512.\n",
        "                random_state : Integer, optional\n",
        "                    RandomState instance or None, optional (default=None)\n",
        "                    If Integer, random_state is the seed used by the random number generator;\n",
        "                random_optimizor : bool, optional\n",
        "                    If False, all models use adam optimizer. If True, all models use random optimizers. it will default to True\n",
        "                dropout: Float, optional\n",
        "                    between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "    if len(x_train) != len(y_train):\n",
        "        raise ValueError('shape of x_train and y_train must be equal'\n",
        "                         'The x_train has ' + str(len(x_train)) +\n",
        "                         'The x_train has' +\n",
        "                         str(len(y_train)))\n",
        "\n",
        "    if len(x_test) != len(y_test):\n",
        "        raise ValueError('shape of x_test and y_test must be equal '\n",
        "                         'The x_train has ' + str(len(x_test)) +\n",
        "                         'The y_test has ' +\n",
        "                         str(len(y_test)))\n",
        "\n",
        "    np.random.seed(random_state)\n",
        "    G.setup()\n",
        "    y_proba = []\n",
        "\n",
        "    score = []\n",
        "    history_ = []\n",
        "    if sparse_categorical:\n",
        "        number_of_classes = np.max(y_train)+1\n",
        "    else:\n",
        "        number_of_classes = np.shape(y_train)[0]\n",
        "\n",
        "    i =0\n",
        "    while i < random_deep[0]:\n",
        "        try:\n",
        "            print(\"DNN \", i, \"\\n\")\n",
        "            model_DNN, model_tmp = BuildModel.Build_Model_DNN_Image(shape,\n",
        "                                                                    number_of_classes,\n",
        "                                                                    sparse_categorical,\n",
        "                                                                    min_hidden_layer_dnn,\n",
        "                                                                    max_hidden_layer_dnn,\n",
        "                                                                    min_nodes_dnn,\n",
        "                                                                    max_nodes_dnn,\n",
        "                                                                    random_optimizor,\n",
        "                                                                    dropout)\n",
        "\n",
        "\n",
        "            filepath = \"weights\\weights_DNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath,\n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            history = model_DNN.fit(x_train, y_train,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    epochs=epochs[0],\n",
        "                                    batch_size=batch_size,\n",
        "                                    callbacks=callbacks_list,\n",
        "                                    verbose=2)\n",
        "            history_.append(history)\n",
        "            model_tmp.load_weights(filepath)\n",
        "\n",
        "            if sparse_categorical == 0:\n",
        "                model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                                  optimizer='adam',\n",
        "                                  metrics=['accuracy'])\n",
        "            else:\n",
        "                model_tmp.compile(loss='categorical_crossentropy',\n",
        "                                  optimizer='adam',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "            y_pr = model_tmp.predict_classes(x_test, batch_size=batch_size)\n",
        "            y_proba.append(np.array(y_pr))\n",
        "            score.append(accuracy_score(y_test, y_pr))\n",
        "            i = i + 1\n",
        "            del model_tmp\n",
        "            del model_DNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \"try to re-generate an other model\")\n",
        "            if max_hidden_layer_dnn > 3:\n",
        "                max_hidden_layer_dnn -= 1\n",
        "            if max_nodes_dnn > 256:\n",
        "                max_nodes_dnn -= 8\n",
        "\n",
        "\n",
        "    i =0\n",
        "    while i < random_deep[1]:\n",
        "        try:\n",
        "            print(\"RNN \", i, \"\\n\")\n",
        "            model_RNN, model_tmp = BuildModel.Build_Model_RNN_Image(shape,\n",
        "                                                                    number_of_classes,\n",
        "                                                                    sparse_categorical,\n",
        "                                                                    min_nodes_rnn,\n",
        "                                                                    max_nodes_rnn,\n",
        "                                                                    random_optimizor,\n",
        "                                                                    dropout)\n",
        "\n",
        "            filepath = \"weights\\weights_RNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath,\n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            history = model_RNN.fit(x_train, y_train,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    epochs=epochs[1],\n",
        "                                    batch_size=batch_size,\n",
        "                                    verbose=2,\n",
        "                                    callbacks=callbacks_list)\n",
        "\n",
        "            model_tmp.load_weights(filepath)\n",
        "            model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                              optimizer='rmsprop',\n",
        "                              metrics=['accuracy'])\n",
        "            history_.append(history)\n",
        "\n",
        "            y_pr = model_tmp.predict(x_test, batch_size=batch_size)\n",
        "            y_pr = np.argmax(y_pr, axis=1)\n",
        "            y_proba.append(np.array(y_pr))\n",
        "            score.append(accuracy_score(y_test, y_pr))\n",
        "            i = i+1\n",
        "            del model_tmp\n",
        "            del model_RNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \" try to re-generate another model\")\n",
        "            if max_hidden_layer_rnn > 3:\n",
        "                max_hidden_layer_rnn -= 1\n",
        "            if max_nodes_rnn > 64:\n",
        "                max_nodes_rnn -= 2\n",
        "\n",
        "    # reshape to be [samples][pixels][width][height]\n",
        "    i=0\n",
        "    while i < random_deep[2]:\n",
        "        try:\n",
        "            print(\"CNN \", i, \"\\n\")\n",
        "            model_CNN, model_tmp = BuildModel.Build_Model_CNN_Image(shape,\n",
        "                                                                    number_of_classes,\n",
        "                                                                    sparse_categorical,\n",
        "                                                                    min_hidden_layer_cnn,\n",
        "                                                                    max_hidden_layer_cnn,\n",
        "                                                                    min_nodes_cnn,\n",
        "                                                                    max_nodes_cnn,\n",
        "                                                                    random_optimizor,\n",
        "                                                                    dropout)\n",
        "\n",
        "            filepath = \"weights\\weights_CNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath, \n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1, \n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            history = model_CNN.fit(x_train, y_train,\n",
        "                                    validation_data=(x_test, y_test),\n",
        "                                    epochs=epochs[2],\n",
        "                                    batch_size=batch_size,\n",
        "                                    callbacks=callbacks_list,\n",
        "                                    verbose=2)\n",
        "            history_.append(history)\n",
        "            model_tmp.load_weights(filepath)\n",
        "            model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                              optimizer='adam',\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "            y_pr = model_tmp.predict_classes(x_test, batch_size=batch_size)\n",
        "            y_proba.append(np.array(y_pr))\n",
        "            score.append(accuracy_score(y_test, y_pr))\n",
        "            i = i+1\n",
        "            del model_tmp\n",
        "            del model_CNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \" try to re-generate another model\")\n",
        "            if max_hidden_layer_cnn > 5:\n",
        "                max_hidden_layer_cnn -= 1\n",
        "            if max_nodes_cnn > 128:\n",
        "                max_nodes_cnn -= 2\n",
        "                min_nodes_cnn -= 1\n",
        "\n",
        "\n",
        "\n",
        "    y_proba = np.array(y_proba).transpose()\n",
        "    print(y_proba.shape)\n",
        "    final_y = []\n",
        "    for i in range(0, y_proba.shape[0]):\n",
        "        a = np.array(y_proba[i, :])\n",
        "        a = collections.Counter(a).most_common()[0][0]\n",
        "        final_y.append(a)\n",
        "    F_score = accuracy_score(y_test, final_y)\n",
        "    F1 = f1_score(y_test, final_y, average='micro')\n",
        "    F2 = f1_score(y_test, final_y, average='macro')\n",
        "    F3 = f1_score(y_test, final_y, average='weighted')\n",
        "    cnf_matrix = confusion_matrix(y_test, final_y)\n",
        "    # Compute confusion matrix\n",
        "    np.set_printoptions(precision=2)\n",
        "    if plot:\n",
        "        # Plot non-normalized confusion matrix\n",
        "        classes = list(range(0,np.max(y_test)+1))\n",
        "        Plot.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                         title='Confusion matrix, without normalization')\n",
        "        Plot.plot_confusion_matrix(cnf_matrix, classes=classes,normalize=True,\n",
        "                              title='Confusion matrix, without normalization')\n",
        "\n",
        "    if plot:\n",
        "        Plot.RMDL_epoch(history_)\n",
        "\n",
        "    print(y_proba.shape)\n",
        "    print(\"Accuracy of\",len(score),\"models:\",score)\n",
        "    print(\"Accuracy:\",np.amax(score))\n",
        "    print(\"Accuracy F Score:\",F_score)\n",
        "    print(\"F1_Micro:\",F1)\n",
        "    print(\"F1_Macro:\",F2)\n",
        "    print(\"F1_weighted:\",F3)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting RMDL\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/1c/7911d9b8ea3a95983d19720560963b3b809af7308a46a111756606ed928f/RMDL-1.0.8-py2.py3-none-any.whl (44kB)\n",
            "\r\u001b[K     |                        | 10kB 20.1MB/s eta 0:00:01\r\u001b[K     |                 | 20kB 11.6MB/s eta 0:00:01\r\u001b[K     |          | 30kB 8.8MB/s eta 0:00:01\r\u001b[K     |  | 40kB 8.1MB/s eta 0:00:01\r\u001b[K     || 51kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from RMDL) (2.4.1)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from RMDL) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from RMDL) (1.19.5)\n",
            "Requirement already satisfied: keras>=2.0.9 in /usr/local/lib/python3.6/dist-packages (from RMDL) (2.4.3)\n",
            "Requirement already satisfied: scikit-learn>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from RMDL) (0.22.2.post1)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.6/dist-packages (from RMDL) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from RMDL) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from RMDL) (3.2.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (3.3.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.1.2)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (0.36.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.1.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.32.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (3.12.4)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (0.3.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (2.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (2.4.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.12)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (1.12.1)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->RMDL) (0.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->RMDL) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->RMDL) (2018.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.0.9->RMDL) (3.13)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.0->RMDL) (1.0.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->RMDL) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->RMDL) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->RMDL) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow->RMDL) (53.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (1.25.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow->RMDL) (0.4.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->RMDL) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->RMDL) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->RMDL) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow->RMDL) (2020.12.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow->RMDL) (3.4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->RMDL) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->RMDL) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->RMDL) (4.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->RMDL) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow->RMDL) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow->RMDL) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow->RMDL) (3.1.0)\n",
            "Installing collected packages: RMDL\n",
            "Successfully installed RMDL-1.0.8\n",
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
            "sys.version_info(major=3, minor=6, micro=9, releaselevel='final', serial=0)\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "377ni0W2NPjp"
      },
      "source": [
        "#TEXT CLASSIFICATION\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import collections\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from RMDL import BuildModel as BuildModel\n",
        "from RMDL.Download import Download_Glove as GloVe\n",
        "from RMDL import text_feature_extraction as txt\n",
        "from RMDL import Global as G\n",
        "from RMDL import Plot as Plot\n",
        "\n",
        "\n",
        "def Text_Classification(x_train, y_train, x_test,  y_test, batch_size=128,\n",
        "                        EMBEDDING_DIM=50,MAX_SEQUENCE_LENGTH = 500, MAX_NB_WORDS = 75000,\n",
        "                        GloVe_dir=\"\", GloVe_file = \"glove.6B.50d.txt\",\n",
        "                        sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500],  plot=False,\n",
        "                        min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n",
        "                        min_hidden_layer_rnn=1, max_hidden_layer_rnn=5, min_nodes_rnn=32,  max_nodes_rnn=128,\n",
        "                        min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n",
        "                        random_state=42, random_optimizor=True, dropout=0.5,no_of_classes=0):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Text_Classification(x_train, y_train, x_test,  y_test, batch_size=128,\n",
        "                        EMBEDDING_DIM=50,MAX_SEQUENCE_LENGTH = 500, MAX_NB_WORDS = 75000,\n",
        "                        GloVe_dir=\"\", GloVe_file = \"glove.6B.50d.txt\",\n",
        "                        sparse_categorical=True, random_deep=[3, 3, 3], epochs=[500, 500, 500],  plot=False,\n",
        "                        min_hidden_layer_dnn=1, max_hidden_layer_dnn=8, min_nodes_dnn=128, max_nodes_dnn=1024,\n",
        "                        min_hidden_layer_rnn=1, max_hidden_layer_rnn=5, min_nodes_rnn=32,  max_nodes_rnn=128,\n",
        "                        min_hidden_layer_cnn=3, max_hidden_layer_cnn=10, min_nodes_cnn=128, max_nodes_cnn=512,\n",
        "                        random_state=42, random_optimizor=True, dropout=0.5):\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "            batch_size : Integer, , optional\n",
        "                Number of samples per gradient update. If unspecified, it will default to 128\n",
        "            MAX_NB_WORDS: int, optional\n",
        "                Maximum number of unique words in datasets, it will default to 75000.\n",
        "            GloVe_dir: String, optional\n",
        "                Address of GloVe or any pre-trained directory, it will default to null which glove.6B.zip will be download.\n",
        "            GloVe_dir: String, optional\n",
        "                Which version of GloVe or pre-trained word emending will be used, it will default to glove.6B.50d.txt.\n",
        "                NOTE: if you use other version of GloVe EMBEDDING_DIM must be same dimensions.\n",
        "            sparse_categorical: bool.\n",
        "                When target's dataset is (n,1) should be True, it will default to True.\n",
        "            random_deep: array of int [3], optional\n",
        "                Number of ensembled model used in RMDL random_deep[0] is number of DNN, random_deep[1] is number of RNN, random_deep[0] is number of CNN, it will default to [3, 3, 3].\n",
        "            epochs: array of int [3], optional\n",
        "                Number of epochs in each ensembled model used in RMDL epochs[0] is number of epochs used in DNN, epochs[1] is number of epochs used in RNN, epochs[0] is number of epochs used in CNN, it will default to [500, 500, 500].\n",
        "            plot: bool, optional\n",
        "                True: shows confusion matrix and accuracy and loss\n",
        "            min_hidden_layer_dnn: Integer, optional\n",
        "                Lower Bounds of hidden layers of DNN used in RMDL, it will default to 1.\n",
        "            max_hidden_layer_dnn: Integer, optional\n",
        "                Upper bounds of hidden layers of DNN used in RMDL, it will default to 8.\n",
        "            min_nodes_dnn: Integer, optional\n",
        "                Lower bounds of nodes in each layer of DNN used in RMDL, it will default to 128.\n",
        "            max_nodes_dnn: Integer, optional\n",
        "                Upper bounds of nodes in each layer of DNN used in RMDL, it will default to 1024.\n",
        "            min_hidden_layer_rnn: Integer, optional\n",
        "                Lower Bounds of hidden layers of RNN used in RMDL, it will default to 1.\n",
        "            min_hidden_layer_rnn: Integer, optional\n",
        "                Upper Bounds of hidden layers of RNN used in RMDL, it will default to 5.\n",
        "            min_nodes_rnn: Integer, optional\n",
        "                Lower bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 32.\n",
        "            max_nodes_rnn: Integer, optional\n",
        "                Upper bounds of nodes (LSTM or GRU) in each layer of RNN used in RMDL, it will default to 128.\n",
        "            min_hidden_layer_cnn: Integer, optional\n",
        "                Lower Bounds of hidden layers of CNN used in RMDL, it will default to 3.\n",
        "            max_hidden_layer_cnn: Integer, optional\n",
        "                Upper Bounds of hidden layers of CNN used in RMDL, it will default to 10.\n",
        "            min_nodes_cnn: Integer, optional\n",
        "                Lower bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 128.\n",
        "            min_nodes_cnn: Integer, optional\n",
        "                Upper bounds of nodes (2D convolution layer) in each layer of CNN used in RMDL, it will default to 512.\n",
        "            random_state : Integer, optional\n",
        "                RandomState instance or None, optional (default=None)\n",
        "                If Integer, random_state is the seed used by the random number generator;\n",
        "            random_optimizor : bool, optional\n",
        "                If False, all models use adam optimizer. If True, all models use random optimizers. it will default to True\n",
        "            dropout: Float, optional\n",
        "                between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.\n",
        "\n",
        "    \"\"\"\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "\n",
        "    glove_directory = GloVe_dir\n",
        "    GloVe_file = GloVe_file\n",
        "\n",
        "    print(\"Done1\")\n",
        "\n",
        "    GloVe_needed = random_deep[1] != 0 or random_deep[2] != 0\n",
        "    \n",
        "    # example_input  = [0,1,3]\n",
        "    # example_output :\n",
        "    # \n",
        "    # [[1 0 0 0]\n",
        "    #  [0 1 0 0]\n",
        "    #  [0 0 0 1]]\n",
        "    \n",
        "    def one_hot_encoder(value, label_data_):\n",
        "\n",
        "        label_data_[value] = 1\n",
        "\n",
        "        return label_data_\n",
        "\n",
        "    def _one_hot_values(labels_data):\n",
        "        encoded = [0] * len(labels_data)\n",
        "\n",
        "        for index_no, value in enumerate(labels_data):\n",
        "            max_value = [0] * (np.max(labels_data) + 1)\n",
        "\n",
        "            encoded[index_no] = one_hot_encoder(value, max_value)\n",
        "\n",
        "        return np.array(encoded)\n",
        "\n",
        "    if not isinstance(y_train[0], list) and not isinstance(y_train[0], np.ndarray) and not sparse_categorical:\n",
        "        #checking if labels are one hot or not otherwise dense_layer will give shape error \n",
        "        \n",
        "        print(\"converted_into_one_hot\")\n",
        "        y_train = _one_hot_values(y_train)\n",
        "        y_test = _one_hot_values(y_test)\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if GloVe_needed:\n",
        "        if glove_directory == \"\":\n",
        "            GloVe_directory = GloVe.download_and_extract()\n",
        "            GloVe_DIR = os.path.join(GloVe_directory, GloVe_file)\n",
        "        else:\n",
        "            GloVe_DIR = os.path.join(glove_directory, GloVe_file)\n",
        "\n",
        "        if not os.path.isfile(GloVe_DIR):\n",
        "            print(\"Could not find %s Set GloVe Directory in Global.py \", GloVe)\n",
        "            exit()\n",
        "\n",
        "    G.setup()\n",
        "    if random_deep[0] != 0:\n",
        "        x_train_tfidf, x_test_tfidf = txt.loadData(x_train, x_test,MAX_NB_WORDS=MAX_NB_WORDS)\n",
        "    if random_deep[1] != 0 or random_deep[2] != 0 :\n",
        "        print(GloVe_DIR)\n",
        "        x_train_embedded, x_test_embedded, word_index, embeddings_index = txt.loadData_Tokenizer(x_train, x_test,GloVe_DIR,MAX_NB_WORDS,MAX_SEQUENCE_LENGTH,EMBEDDING_DIM)\n",
        "\n",
        "    del x_train\n",
        "    del x_test\n",
        "    gc.collect()\n",
        "\n",
        "    y_pr = []\n",
        "    History = []\n",
        "    score = []\n",
        "\n",
        "    if no_of_classes==0:\n",
        "        #checking no_of_classes\n",
        "        #np.max(data)+1 will not work for one_hot encoding labels\n",
        "        if sparse_categorical:\n",
        "            number_of_classes = np.max(y_train) + 1\n",
        "        else:\n",
        "            number_of_classes = len(y_train[0])\n",
        "    else:\n",
        "        number_of_classes = no_of_classes\n",
        "    print(number_of_classes)\n",
        "\n",
        "\n",
        "    i = 0\n",
        "    while i < random_deep[0]:\n",
        "        # model_DNN.append(Sequential())\n",
        "        try:\n",
        "            print(\"DNN \" + str(i))\n",
        "            filepath = \"weights\\weights_DNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath,\n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            model_DNN, model_tmp = BuildModel.Build_Model_DNN_Text(x_train_tfidf.shape[1],\n",
        "                                                                   number_of_classes,\n",
        "                                                                   sparse_categorical,\n",
        "                                                                   min_hidden_layer_dnn,\n",
        "                                                                   max_hidden_layer_dnn,\n",
        "                                                                   min_nodes_dnn,\n",
        "                                                                   max_nodes_dnn,\n",
        "                                                                   random_optimizor,\n",
        "                                                                   dropout)\n",
        "            model_history = model_DNN.fit(x_train_tfidf, y_train,\n",
        "                              validation_data=(x_test_tfidf, y_test),\n",
        "                              epochs=epochs[0],\n",
        "                              batch_size=batch_size,\n",
        "                              callbacks=callbacks_list,\n",
        "                              verbose=2)\n",
        "            History.append(model_history)\n",
        "\n",
        "            model_tmp.load_weights(filepath)\n",
        "            if sparse_categorical:\n",
        "                model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                                  optimizer='adam',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "                y_pr_ = model_tmp.predict_classes(x_test_tfidf,\n",
        "                                                  batch_size=batch_size)\n",
        "                y_pr.append(np.array(y_pr_))\n",
        "                score.append(accuracy_score(y_test, y_pr_))\n",
        "            else:\n",
        "                model_tmp.compile(loss='categorical_crossentropy',\n",
        "                                  optimizer='adam',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "                y_pr_ = model_tmp.predict(x_test_tfidf,\n",
        "                                          batch_size=batch_size)\n",
        "\n",
        "                y_pr_ = np.argmax(y_pr_, axis=1)\n",
        "                y_pr.append(np.array(y_pr_))\n",
        "                y_test_temp = np.argmax(y_test, axis=1)\n",
        "                score.append(accuracy_score(y_test_temp, y_pr_))\n",
        "            # print(y_proba)\n",
        "            i += 1\n",
        "            del model_tmp\n",
        "            del model_DNN\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            print(\"Check the Error \\n {} \".format(e))\n",
        "\n",
        "            print(\"Error in model\", i, \"try to re-generate another model\")\n",
        "            if max_hidden_layer_dnn > 3:\n",
        "                max_hidden_layer_dnn -= 1\n",
        "            if max_nodes_dnn > 256:\n",
        "                max_nodes_dnn -= 8\n",
        "\n",
        "    try:\n",
        "        del x_train_tfidf\n",
        "        del x_test_tfidf\n",
        "        gc.collect()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    i=0\n",
        "    while i < random_deep[1]:\n",
        "        try:\n",
        "            print(\"RNN \" + str(i))\n",
        "            filepath = \"weights\\weights_RNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath,\n",
        "                                         monitor='val_accuracy',\n",
        "                                         verbose=1,\n",
        "                                         save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            model_RNN, model_tmp = BuildModel.Build_Model_RNN_Text(word_index,\n",
        "                                                                   embeddings_index,\n",
        "                                                                   number_of_classes,\n",
        "                                                                   MAX_SEQUENCE_LENGTH,\n",
        "                                                                   EMBEDDING_DIM,\n",
        "                                                                   sparse_categorical,\n",
        "                                                                   min_hidden_layer_rnn,\n",
        "                                                                   max_hidden_layer_rnn,\n",
        "                                                                   min_nodes_rnn,\n",
        "                                                                   max_nodes_rnn,\n",
        "                                                                   random_optimizor,\n",
        "                                                                   dropout)\n",
        "\n",
        "            model_history = model_RNN.fit(x_train_embedded, y_train,\n",
        "                              validation_data=(x_test_embedded, y_test),\n",
        "                              epochs=epochs[1],\n",
        "                              batch_size=batch_size,\n",
        "                              callbacks=callbacks_list,\n",
        "                              verbose=2)\n",
        "            History.append(model_history)\n",
        "\n",
        "            if sparse_categorical:\n",
        "                model_tmp.load_weights(filepath)\n",
        "                model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                                  optimizer='rmsprop',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "                y_pr_ = model_tmp.predict_classes(x_test_embedded, batch_size=batch_size)\n",
        "                y_pr.append(np.array(y_pr_))\n",
        "                score.append(accuracy_score(y_test, y_pr_))\n",
        "            else:\n",
        "                model_tmp.load_weights(filepath)\n",
        "                model_tmp.compile(loss='categorical_crossentropy',\n",
        "                                  optimizer='rmsprop',\n",
        "                                  metrics=['accuracy'])\n",
        "                y_pr_ = model_tmp.predict(x_test_embedded, batch_size=batch_size)\n",
        "                y_pr_ = np.argmax(y_pr_, axis=1)\n",
        "                y_pr.append(np.array(y_pr_))\n",
        "                y_test_temp = np.argmax(y_test, axis=1)\n",
        "                score.append(accuracy_score(y_test_temp, y_pr_))\n",
        "            i += 1\n",
        "            del model_tmp\n",
        "            del model_RNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \"try to re-generate another model\")\n",
        "            if max_hidden_layer_rnn > 3:\n",
        "                max_hidden_layer_rnn -= 1\n",
        "            if max_nodes_rnn > 64:\n",
        "                max_nodes_rnn -= 2\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    i = 0\n",
        "    while i < random_deep[2]:\n",
        "        try:\n",
        "            print(\"CNN \" + str(i))\n",
        "\n",
        "            model_CNN, model_tmp = BuildModel.Build_Model_CNN_Text(word_index,\n",
        "                                                                   embeddings_index,\n",
        "                                                                   number_of_classes,\n",
        "                                                                   MAX_SEQUENCE_LENGTH,\n",
        "                                                                   EMBEDDING_DIM,\n",
        "                                                                   sparse_categorical,\n",
        "                                                                   min_hidden_layer_cnn,\n",
        "                                                                   max_hidden_layer_cnn,\n",
        "                                                                   min_nodes_cnn,\n",
        "                                                                   max_nodes_cnn,\n",
        "                                                                   random_optimizor,\n",
        "                                                                   dropout)\n",
        "\n",
        "\n",
        "\n",
        "            filepath = \"weights\\weights_CNN_\" + str(i) + \".hdf5\"\n",
        "            checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True,\n",
        "                                         mode='max')\n",
        "            callbacks_list = [checkpoint]\n",
        "\n",
        "            model_history = model_CNN.fit(x_train_embedded, y_train,\n",
        "                                          validation_data=(x_test_embedded, y_test),\n",
        "                                          epochs=epochs[2],\n",
        "                                          batch_size=batch_size,\n",
        "                                          callbacks=callbacks_list,\n",
        "                                          verbose=2)\n",
        "            History.append(model_history)\n",
        "\n",
        "            model_tmp.load_weights(filepath)\n",
        "            if sparse_categorical:\n",
        "                model_tmp.compile(loss='sparse_categorical_crossentropy',\n",
        "                                  optimizer='rmsprop',\n",
        "                                  metrics=['accuracy'])\n",
        "            else:\n",
        "                model_tmp.compile(loss='categorical_crossentropy',\n",
        "                                  optimizer='rmsprop',\n",
        "                                  metrics=['accuracy'])\n",
        "\n",
        "            y_pr_ = model_tmp.predict(x_test_embedded, batch_size=batch_size)\n",
        "            y_pr_ = np.argmax(y_pr_, axis=1)\n",
        "            y_pr.append(np.array(y_pr_))\n",
        "\n",
        "            if sparse_categorical:\n",
        "                score.append(accuracy_score(y_test, y_pr_))\n",
        "            else:\n",
        "                y_test_temp = np.argmax(y_test, axis=1)\n",
        "                score.append(accuracy_score(y_test_temp, y_pr_))\n",
        "            i += 1\n",
        "\n",
        "            del model_tmp\n",
        "            del model_CNN\n",
        "            gc.collect()\n",
        "        except:\n",
        "            print(\"Error in model\", i, \"try to re-generate an other model\")\n",
        "            if max_hidden_layer_cnn > 5:\n",
        "                max_hidden_layer_cnn -= 1\n",
        "            if max_nodes_cnn > 128:\n",
        "                max_nodes_cnn -= 2\n",
        "                min_nodes_cnn -= 1\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    y_proba = np.array(y_pr).transpose()\n",
        "\n",
        "    final_y = []\n",
        "\n",
        "    for i in range(0, y_proba.shape[0]):\n",
        "        a = np.array(y_proba[i, :])\n",
        "        a = collections.Counter(a).most_common()[0][0]\n",
        "        final_y.append(a)\n",
        "    if sparse_categorical:\n",
        "        F_score = accuracy_score(y_test, final_y)\n",
        "        F1 = precision_recall_fscore_support(y_test, final_y, average='micro')\n",
        "        F2 = precision_recall_fscore_support(y_test, final_y, average='macro')\n",
        "        F3 = precision_recall_fscore_support(y_test, final_y, average='weighted')\n",
        "        cnf_matrix = confusion_matrix(y_test, final_y)\n",
        "        # Compute confusion matrix\n",
        "        # Plot non-normalized confusion matrix\n",
        "\n",
        "        if plot:\n",
        "            classes = list(range(0, np.max(y_test)+1))\n",
        "            Plot.plot_confusion_matrix(cnf_matrix, classes=classes,\n",
        "                                       title='Confusion matrix, without normalization')\n",
        "\n",
        "            # Plot normalized confusion matrix\n",
        "\n",
        "            Plot.plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True,\n",
        "                                       title='Normalized confusion matrix')\n",
        "    else:\n",
        "        y_test_temp = np.argmax(y_test, axis=1)\n",
        "        F_score = accuracy_score(y_test_temp, final_y)\n",
        "        F1 = precision_recall_fscore_support(y_test_temp, final_y, average='micro')\n",
        "        F2 = precision_recall_fscore_support(y_test_temp, final_y, average='macro')\n",
        "        F3 = precision_recall_fscore_support(y_test_temp, final_y, average='weighted')\n",
        "    if plot:\n",
        "        Plot.RMDL_epoch(History)\n",
        "    print(y_proba.shape)\n",
        "    print(\"Accuracy of\",len(score),\"models:\",score)\n",
        "    print(\"Accuracy:\",np.amax(score))\n",
        "    # print(\"Accuracy:\",F_score)\n",
        "    print(\"F1_Micro:\",F1)\n",
        "    print(\"F1_Macro:\",F2)\n",
        "    print(\"F1_weighted:\",F3)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t43THPpnNd-b",
        "outputId": "63b1c549-7393-46c3-d083-d9ab04908bb9"
      },
      "source": [
        "# TEXT FEATURE EXTRACTION\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "nltk.download(\"stopwords\")\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "\n",
        "def transliterate(line):\n",
        "    cedilla2latin = [[u'', u'A'], [u'', u'a'], [u'', u'C'], [u'', u'c'], [u'', u'S'], [u'', u's']]\n",
        "    tr = dict([(a[0], a[1]) for (a) in cedilla2latin])\n",
        "    new_line = \"\"\n",
        "    for letter in line:\n",
        "        if letter in tr:\n",
        "            new_line += tr[letter]\n",
        "        else:\n",
        "            new_line += letter\n",
        "    return new_line\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTnyErA5NiX2",
        "outputId": "0dfd629f-2aac-4632-be95-28071cde1c21"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "from RMDL import text_feature_extraction as txt\n",
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "print(\"Load IMDB dataset....\")\n",
        "MAX_NB_WORDS = 7500\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=MAX_NB_WORDS)\n",
        "print(len(X_train))\n",
        "print(y_test)\n",
        "word_index = imdb.get_word_index()\n",
        "index_word = {v: k for k, v in word_index.items()}\n",
        "X_train = [txt.text_cleaner(' '.join(index_word.get(w) for w in x)) for x in X_train]\n",
        "X_test = [txt.text_cleaner(' '.join(index_word.get(w) for w in x)) for x in X_test]\n",
        "X_train = np.array(X_train)\n",
        "X_train = np.array(X_train).ravel()\n",
        "print(X_train.shape)\n",
        "X_test = np.array(X_test)\n",
        "X_test = np.array(X_test).ravel()\n",
        "\n",
        "batch_size = 100\n",
        "sparse_categorical = 0\n",
        "n_epochs = [5, 5, 5]  ## DNN--RNN-CNN\n",
        "Random_Deep = [2, 2, 2]  ## DNN--RNN-CNN\n",
        "\n",
        "Text_Classification(X_train, y_train, X_test, y_test,\n",
        "                     batch_size=batch_size,\n",
        "                     sparse_categorical=sparse_categorical,\n",
        "                     random_deep=Random_Deep,\n",
        "                     epochs=n_epochs)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load IMDB dataset....\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "25000\n",
            "[0 1 1 ... 0 0 0]\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "Done1\n",
            "converted_into_one_hot\n",
            ".\\Glove/glove.6B.zip\n",
            "tf-idf with 7335 features\n",
            "/content/.\\Glove/glove.6B.50d.txt\n",
            "Found 7497 unique tokens.\n",
            "(50000, 500)\n",
            "Total 400000 word vectors.\n",
            "2\n",
            "DNN 0\n",
            "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f28441f7780>\n",
            "Epoch 1/5\n",
            "250/250 - 3s - loss: 0.6933 - accuracy: 0.5017 - val_loss: 0.6931 - val_accuracy: 0.5098\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.50976, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 2/5\n",
            "250/250 - 1s - loss: 0.6932 - accuracy: 0.5032 - val_loss: 0.6929 - val_accuracy: 0.5252\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.50976 to 0.52524, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 3/5\n",
            "250/250 - 1s - loss: 0.6929 - accuracy: 0.5122 - val_loss: 0.6928 - val_accuracy: 0.5400\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.52524 to 0.54000, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 4/5\n",
            "250/250 - 1s - loss: 0.6929 - accuracy: 0.5085 - val_loss: 0.6926 - val_accuracy: 0.5526\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.54000 to 0.55260, saving model to weights\\weights_DNN_0.hdf5\n",
            "Epoch 5/5\n",
            "250/250 - 1s - loss: 0.6928 - accuracy: 0.5126 - val_loss: 0.6925 - val_accuracy: 0.5685\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.55260 to 0.56852, saving model to weights\\weights_DNN_0.hdf5\n",
            "DNN 1\n",
            "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f2845d2fef0>\n",
            "Epoch 1/5\n",
            "250/250 - 2s - loss: 0.6933 - accuracy: 0.4974 - val_loss: 0.6930 - val_accuracy: 0.5094\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.50944, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 2/5\n",
            "250/250 - 1s - loss: 0.6930 - accuracy: 0.5035 - val_loss: 0.6927 - val_accuracy: 0.5348\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.50944 to 0.53484, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 3/5\n",
            "250/250 - 1s - loss: 0.6927 - accuracy: 0.5126 - val_loss: 0.6923 - val_accuracy: 0.5557\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.53484 to 0.55568, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 4/5\n",
            "250/250 - 1s - loss: 0.6923 - accuracy: 0.5262 - val_loss: 0.6920 - val_accuracy: 0.5755\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.55568 to 0.57552, saving model to weights\\weights_DNN_1.hdf5\n",
            "Epoch 5/5\n",
            "250/250 - 1s - loss: 0.6922 - accuracy: 0.5288 - val_loss: 0.6917 - val_accuracy: 0.5903\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.57552 to 0.59028, saving model to weights\\weights_DNN_1.hdf5\n",
            "RNN 0\n",
            "2\n",
            "81\n",
            "WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "<tensorflow.python.keras.optimizer_v2.adagrad.Adagrad object at 0x7f2846077da0>\n",
            "Epoch 1/5\n",
            "250/250 - 614s - loss: 0.6976 - accuracy: 0.4975 - val_loss: 0.6937 - val_accuracy: 0.4969\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.49688, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 2/5\n",
            "250/250 - 612s - loss: 0.6969 - accuracy: 0.4989 - val_loss: 0.6937 - val_accuracy: 0.4992\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.49688 to 0.49920, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 3/5\n",
            "250/250 - 609s - loss: 0.6963 - accuracy: 0.4998 - val_loss: 0.6935 - val_accuracy: 0.5001\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.49920 to 0.50012, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 4/5\n",
            "250/250 - 607s - loss: 0.6960 - accuracy: 0.5007 - val_loss: 0.6932 - val_accuracy: 0.5010\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.50012 to 0.50096, saving model to weights\\weights_RNN_0.hdf5\n",
            "Epoch 5/5\n",
            "250/250 - 602s - loss: 0.6951 - accuracy: 0.5032 - val_loss: 0.6932 - val_accuracy: 0.5017\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.50096 to 0.50168, saving model to weights\\weights_RNN_0.hdf5\n",
            "RNN 1\n",
            "1\n",
            "122\n",
            "WARNING:tensorflow:Layer gru_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer gru_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f2847a00cf8>\n",
            "Epoch 1/5\n",
            "250/250 - 414s - loss: 0.6811 - accuracy: 0.5538 - val_loss: 0.6448 - val_accuracy: 0.6224\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.62236, saving model to weights\\weights_RNN_1.hdf5\n",
            "Epoch 2/5\n",
            "250/250 - 414s - loss: 0.5826 - accuracy: 0.6931 - val_loss: 0.4995 - val_accuracy: 0.7584\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.62236 to 0.75840, saving model to weights\\weights_RNN_1.hdf5\n",
            "Epoch 3/5\n",
            "250/250 - 416s - loss: 0.4642 - accuracy: 0.7883 - val_loss: 0.5518 - val_accuracy: 0.7218\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.75840\n",
            "Epoch 4/5\n",
            "250/250 - 414s - loss: 0.3576 - accuracy: 0.8468 - val_loss: 0.3572 - val_accuracy: 0.8460\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.75840 to 0.84600, saving model to weights\\weights_RNN_1.hdf5\n",
            "Epoch 5/5\n",
            "250/250 - 412s - loss: 0.2858 - accuracy: 0.8834 - val_loss: 0.3013 - val_accuracy: 0.8727\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.84600 to 0.87268, saving model to weights\\weights_RNN_1.hdf5\n",
            "CNN 0\n",
            "Filter   8\n",
            "Node   358\n",
            "<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f2845e8b860>\n",
            "Epoch 1/5\n",
            "250/250 - 49s - loss: 0.9964 - accuracy: 0.4981 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.50000, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 2/5\n",
            "250/250 - 43s - loss: 0.6956 - accuracy: 0.5071 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.50000\n",
            "Epoch 3/5\n",
            "250/250 - 43s - loss: 0.6895 - accuracy: 0.5378 - val_loss: 0.6722 - val_accuracy: 0.5507\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.50000 to 0.55072, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 4/5\n",
            "250/250 - 43s - loss: 0.5947 - accuracy: 0.6994 - val_loss: 0.5522 - val_accuracy: 0.7624\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.55072 to 0.76240, saving model to weights\\weights_CNN_0.hdf5\n",
            "Epoch 5/5\n",
            "250/250 - 43s - loss: 0.4553 - accuracy: 0.8012 - val_loss: 0.4669 - val_accuracy: 0.7986\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.76240 to 0.79860, saving model to weights\\weights_CNN_0.hdf5\n",
            "CNN 1\n",
            "Filter   5\n",
            "Node   367\n",
            "<tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop object at 0x7f2847e6ea20>\n",
            "Epoch 1/5\n",
            "250/250 - 29s - loss: 1.1766 - accuracy: 0.5019 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.50000, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 2/5\n",
            "250/250 - 27s - loss: 0.6967 - accuracy: 0.4970 - val_loss: 0.6931 - val_accuracy: 0.4993\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.50000\n",
            "Epoch 3/5\n",
            "250/250 - 27s - loss: 0.6900 - accuracy: 0.5490 - val_loss: 0.6719 - val_accuracy: 0.6506\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.50000 to 0.65064, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 4/5\n",
            "250/250 - 27s - loss: 0.6013 - accuracy: 0.7004 - val_loss: 0.5337 - val_accuracy: 0.7547\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.65064 to 0.75472, saving model to weights\\weights_CNN_1.hdf5\n",
            "Epoch 5/5\n",
            "250/250 - 27s - loss: 0.4725 - accuracy: 0.7902 - val_loss: 0.4808 - val_accuracy: 0.8296\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.75472 to 0.82964, saving model to weights\\weights_CNN_1.hdf5\n",
            "(25000, 6)\n",
            "Accuracy of 6 models: [0.56852, 0.59028, 0.50168, 0.87268, 0.7986, 0.82964]\n",
            "Accuracy: 0.87268\n",
            "F1_Micro: (0.78268, 0.78268, 0.7826800000000002, None)\n",
            "F1_Macro: (0.7844032177532179, 0.78268, 0.7823503115649881, None)\n",
            "F1_weighted: (0.7844032177532179, 0.78268, 0.7823503115649882, None)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}